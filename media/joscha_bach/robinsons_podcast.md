

# Transcript

```
there is currently a movement of people that arguably was started by um people
0:06
like ILO yot Kowski and uh others who uh reason themselves into the conclusion
0:13
that when AI is going to happen and that's almost inevitable because it's valuable and it's physically possible
0:20
then it's almost inevitable that it's going to be agentic and if it's agentic it's going to be self-improving and if
0:25
it's self-improving it's going to compete with us and because it's smarter than us it will win against us and we
0:30
will all die and uh because Eliza did not see a big hole in this chain of
0:37
reasoning uh a lot of people who uh have the mental property of believing in
0:43
rational thoughts very very literally freaked out and became started to panic and started a movement against Ai and
0:50
then now try to for instance write regulation that ostensively exists to make AI safer but it's actually designed
0:57
to hamstring AI research in such a that it gets slowed down or stopped that's
1:03
that's is the Hope because they don't believe that you can make AI safe they actually believe that it's almost
1:08
inevitable that AI will get better and better if you continue researching on it so uh it is going to inevitably destroy
1:14
us all hello this is Robinson aart here
1:20
with the introduction to Robinson's podcast number 29 and this episode is with one of the
1:27
most requested guests of all time and that is yosha Bach yosha is a computer
1:33
scientist an artificial intelligence researcher with an extremely active mind
1:39
who is currently working with liquid AI though he's previously done research at
1:44
Intel MIT Harvard and the AI Foundation this episode is quite wide
1:52
ranging we start off by talking about yosi's background though we quickly move
2:00
into a number of other things such as the nature of Consciousness in animals
2:05
and machines various theories of Consciousness like pan psychism
2:11
physicalism dualism and Roger penrose's Theory intelligence and artificial
2:18
intelligence the current wave and future developments of chat GPT and other
2:25
Technologies and then whether advances in AI could spell the end of
2:30
Humanity now full disclosure I know very little about computer science and technology so I'm quite grateful to
2:37
yosha for bearing with me through this otherwise I am dreadfully close to a
2:45
five-star average of reviews on Apple podcast so if you're listening on on
2:51
that platform please leave a review or just type in hit the five star button
2:56
because it would be extremely helpful ALS also there there's a link in the
3:01
description to the patreon where you can get access to adree episodes show notes
3:07
and the ability to ask AMA questions the next installment of which is actually
3:14
slated to come out later this week so now without any further Ado I hope that
3:20
you enjoyed this conversation as much as I enjoyed having it with yosha
3:26
[Music]
Why are Legos Like Computer Programs?
3:33
from listening to you talk and I just looked at your Twitter again before I came here it is so apparent how much you
3:41
love computer science and artificial intelligence it's clearly just a vital part of your life and before we talked
3:47
about those things I wanted to hear a bit about just how it became part of
3:53
your world I started programming very early and in a way I started coding with Legos with Legos yes Legos already have
4:01
this idea that you have a world that is fundamentally discrete made out of relatively simple blocks that you can
4:06
combine without trying to subdue nature but only trying to express your thoughts
4:12
and with Legos your ability to limit your thoughts of course limited by the set of building blocks you have how many
4:17
you have what kind of arrangement of I have what properties I have and so on but they in some sense have blocks and
4:23
slots and by planning ahead you can construct and so there is this fundamental Act of producing an
4:31
expression of your ideas via construction and when computers entered
4:37
my life that was at the age of eight or nine I saw a TV show about computers and
4:43
programming and I was smitten because I thought this is basically like Legos only it's completely abstract so you can
4:49
express any thought that you want as long as you're able to translate it into a general language and you can put
4:55
anything on screen with it that you can imagine and can it can occup occupy any kind of space and there is absolutely no
5:02
limitation in terms of The Bu building blocks that you've got because if you've got all of them you just need to know how to combine them there is absolutely
5:08
the only limitation that you have is the memory of your computer which back then was a real issue and then uh of course
5:15
the speed of the computer which means uh how patient are you going to be in looking at the stuff that you are
5:20
generating if it takes very slow to produce the result that you want to have and then um about a year later I got to
5:28
commodora 64 already had learned coding by that time and pasted my relatives to
5:33
get one to me from Western Germany which was quite difficult back then and uh I
5:40
um had was forced to go very deep relatively quickly because the gor 64 does not even have a command to make a
5:47
line and it's this basic language and the basic language itself is relatively close to machine language it's not that
5:53
far remote and in the way in which you think because there are not that many abstractions available it's relatively
5:58
close to the metal and when you have to learn assembler to produce faster things you really get from the ground up and
6:04
realize that the bottom of your CPU there is an automaton and this automaton allows you to build a representational
6:10
language on it in which you can express anything that you want to the degree that you actually understand it so it's
6:16
a tool to develop your thoughts in extremely rigorous Manner and test them but it's also a tool to build anything
6:22
that you want in this virtual world of the computer and there were some Concepts in uh coming from this world of
6:29
the computer that didn't make a lot of sense to me in the world of mathematics for instance I didn't really understand
6:34
what space is I understood that space is a reference that our mind is building
6:39
because it's basically we would say today that we are training a neural network into our mind that is basically
6:44
a black box that is um it's not completely a black box we have some idea on how V1 V2 V4 and so on represents
6:52
stuff and how they are related to each other but from the inside we don't see any of that right we notice that there
6:58
are textures at some level there are geometries that are moving you can start paying attention to how our mind is
7:05
constructing a space but it's basically just the machine learning tool that is trying to make sense of an extremely
7:11
heterogeneous world and so when I Tred to grasp the concepts that
7:16
mathematicians use for geometry they seem to be quite opaque to me because they're actually quite difficult to
7:21
express in a computational language from the ground app it's not easy to construct geometry it's much easier to construct logic and so this intuition
7:30
that you get in school that geometry is easy because your brain already knows how to do it and you just need to refer
7:35
to it and mathematicians already have these intuitions and they seem to be platonically given and then uh on the
7:41
other hand you have this logical language from which you can build geometry but it's actually hard work and then realizing actually the hard work
7:47
has been done by your brain in a way that you couldn't see and it's not given and algebra is actually much more easy
7:54
than geometry and much more fundamental and the true algebra is in some sense
7:59
code that can express anything and some of what it expresses a tiny bit of it is geometry and geometry happens to be the
8:05
way in which we model the visual world and it's an important part of how we model reality because it's grounding our
8:11
symbolic representations right but uh it's necessary to understand both and I
8:17
wish that mathematics or philosophy would have given me an inroad into this but um in some sense I think that the
8:24
progress of mainstream philosophy in understanding mental representation more or less ends with wienstein oh really
8:32
yeah that long ago yes I don't feel that uh when you are taking a normal
8:37
philosophy curriculum in University and you want to understand how uh the mind works and how mental representation
8:43
works you may get to say the uh debate between foror and Poli about whether the
8:51
mind is representing things in a language or in images but from the perspective of a mainstream computer
8:57
scientist this is a ridiculous uh simplification right because of of course it is a language otherwise you
9:03
couldn't represent it and of course the language represents something that we interpret as images so for instance
9:09
geometry MH so geometry with additional attached semantics and in in some sense
9:15
this is so obvious that uh um case is closed for computer scientists when they
9:21
think about how to build a system that represents reality but uh it's completely wide open for philosophy and
9:28
in some sense um wienstein answered uh this or tried to answer this
9:33
question by saying let's patch English into a logical language so we can do philosophical statements that are true
9:39
and false in it and this is what he expresses in the tractatus and then it's over the course of his life he comes to
9:45
the conclusion that he doesn't see a way to ground meaning in grammatical language and for his understanding
9:52
language is quite similar to what um later on um chsky and um others thought
9:59
of language that it has to be discrete and made out of features objects that
10:05
have only very few properties at a time so it's very low dimensional locally and
10:11
of course this is not uh a necessary condition for a general representational language if you want to build a
10:17
representational language of course you can have objects that have millions of properties if you want right and if you
10:23
want to represent um the world in the new world Network then you need representations where individual ual
10:29
features can have thousands of subfeatures that determine them when you are trying to ground them perceptually
10:36
and this is something that is reflected in the way in which we understand the brain to work when we look at the connectivity between individual neurons
10:42
for instance even though it's not a onetoone mapping for our our listeners who aren't computer scientists I found
10:49
this analogy between programming and Legos uh very fascinating but just to
10:55
make it clear I'm curious what the analog of the Lego the building block is
11:01
for the programmer and then just another question you mentioned a a serious
11:07
interest in vision and spatial representation I'm curious what it was that you were first programming and then
11:13
how it transitioned into artificial intelligence what I was first programing was very simple things so for instance
Using Computer Science to Understand Reality
11:21
uh one of the ideas that I had that what Implement is a dice playing game so where you have to guess the number of
11:27
dice and but somebody else has r or you're trying to get certain combinations of dice and try to beat the
11:33
others and the score that you get for them so I took the rules of EX rules of existing games and translated them into
11:39
a computer program that would evaluate those rules and then uh provide the sequence of causal actions that are
11:46
necessary to turn this into a game and then for instance to automate uh the other players so I could play against
11:52
the other players and then the game would at once be the one that evaluates
11:57
the performance of the other players but also Alo produce the Cal structure of the game and by building toy problems
12:05
like this I got deeper intuitions about how programming actually works and how
12:10
it translates your ideas of causal models of causal reality into code that
12:16
is enacting these causal models and in some sense programming code is causal models and the building blocks of
12:23
programming code are the commands computer commands or the operators in which you express your programs and
12:29
what's interesting is that the programming language do not differ in what they can do they only because they
12:34
all let the computer do exactly the same things they differ in how they allow you to think about what the computer should
12:40
do so they provide different levels and kinds of abstractions to uh ultimately
12:46
make the computer go through a sequence of steps that manipulate bits at the lowest level and at the lowest level
12:52
there are automata in the CPU and you can have different sets of automa that produce the same thing and it's
12:58
something that already was known to wienstein who already knew as as for instance explained in the introduction
13:04
that Russell made to the tracts that you only need the N operator the not end
13:09
operator to build all the entire logic and different ways to build the entire logic but to get to this notion that
13:16
later was known as touring completeness um you can have many many variants of
13:21
such building blocks that allow you to eventually build anything you that you want so it's relatively simple to get such a set of building blocks and this
13:29
toing University was something that uh that wienstein was already aware of and toing with his pupil but wienstein I
13:36
think did not see it as significant enough to spell it out but I think it's one of the big philosophical insights of
13:42
the last century the first philosophical Insight of the last century that is really significant in my view is one of
13:49
the biggest ones uh is um gle's um Insight that um the languages of
13:55
mathematics that have stateless truths that have stateless computation uh are leading into
14:02
contradictions there are certain things that you can specify in these languages but that cannot be implemented and this
14:08
is ultimately what Good's proof is about that in these stateless languages there's always sets of statements that
14:14
can you can express but they cannot be translated into any constructive
14:20
enactment and the solution I think to this is to discard these languages and
14:25
go to languages where your semantics is identical to the implement presentation and this is there are
14:31
computational languages and uh this is the first result that basically the representational languages of
14:37
mathematics are allowing too much they are allowing to construct things that are leading into a contradictions that
14:43
cannot be implemented and so on versus the computation languages can do all the things that before worked at mathematics
14:49
but uh they can only do the stuff that can actually be implemented for instance can exist in a physical universe as we
14:55
understand it and the other one is this universality that the all these computational languages ultimately are
15:02
the same language that they can all express the same thing and that's philosophically very significant because
15:07
it allows us now to generalize the way in which we build models about reality and uh realize that all the models they
15:13
are building that we can possibly build including all our mental representations the echos that we make of reality the
15:19
perceptions that we have stuff that we can talk to that we can refer to are in this domain and that's quite significant
15:26
so uh to answer your question from the beginning the reason why I'm interested in computer science is because I believe
15:32
that it provides an Foundation a representational foundation ultimately also an epistemological Foundation to
15:39
make sense of reality and uh it has left mainstream philosophy in a way in the dust and mainstream philosophy is
15:45
unaware of that and as a result it seems to me as now an outsider to philosophy
15:51
that it's mostly dealing with problems that other philosophers have but not uh
15:56
in the same sense as CT it's not a provid of insights for the everyday working man
16:02
right there was a time when philosophers were trying to produce insights that could be used outside of philosophy and
16:07
I don't see this very much in today's philosophy today's philosophy is mostly serving other philosophers does this
16:14
universality you were talking about a few minutes ago the Second Great Insight does it connect also to questions of
16:23
physical multiple realizability the idea of multiple realizability has to do with with the
16:29
fact that we can make bottles of things that are stable causal patterns even if the substrate changes in particular ways
16:36
right so for instance you can uh let the same program run on different computers with different architectures as long as
16:42
there certain specifications M that allow the software to run and anly you
16:48
could say that observe that your mind is stable even if you disturb the brain to some degree right so you can kill a
16:54
number of neurons and your mind is going to entrain new ones you can change the neural chemistry VIs C parameters or the
16:59
temperature of your brain and so the physical makeup of the subset is changing but the function that is
17:04
enacted by the subset at a certain level of abstraction a certain course graining Remains the Same right and uh this is of
17:11
course a source of big confusion are these course grain patterns real or not MH right and or do they produce downward
17:19
coration and so on of course when we are talking about these models of reality we
17:24
always talking about models and I think emergence or supervenience of fundamental relationships between
17:30
different descriptions so you have a description at the level of say activations of cells and the description
17:36
at the level of mental representations but you can change many of the cells and
17:41
many of the activations of the cell and yet functionally having the representations the same having the function that is implemented by them
17:48
because the representation supervenes on the cells yes but it's not necessarily
17:53
the same thing as uh different programming languages express the same thing so this universality is a Sly
17:59
different property it describes the fact that ultimately what computers are doing is that they describe causal structure
18:07
via State Transitions and you can group these State Transitions and uh build
18:12
relationships to them in different ways so you can project them into different description languages but the ca of
18:18
pattern can be the same and so this universality says that all these description languages once they allow
18:24
you to describe one of those computers one of those systems can be used to describe all the others so for instance
18:31
um the church during thesis it's a thesis is not approved so it's a conjecture but it says that um whenever
18:39
you have a machine like a computer that can Ena Java you can use this Java programming language to simulate another
18:45
computer that runs C for instance and vice versa and uh you can impl all of
18:51
these computers on touring machine and vice versa of course the graphics are going to suck on the tour machine yeah
18:57
so getting back though to to your story and toward talking a bit more about
19:03
artificial intelligence when did that become part of your work and interests
19:09
because it started with dice that we're not at artificial intelligence yet so when I uh once I realized that I can
Is Reality a Simulation?
19:15
construct anything that I want the question is what is it that you want to build what do you want you put behind the screen and the obvious answer is an
19:23
entire universe full of Minds that we can actually talk to a simulation yeah but uh maybe that'll come later the
19:30
there also this question is this uh what we are in a simulation so when I look at the world I do I perceive more than a
19:37
screen some surface some kind of um systemic boundary uh that I use to make
19:42
my models over over which I project my model of reality and so to me in some sense um reality is not different from a
19:50
screen that means uh there is a structure behind the screen and I cannot know what structure it is the only thing
19:56
I know is um it I can describe it by State Transitions and it produces regular patterns so the universe is some
20:02
kind of pattern generator and because all the pattern generators appear to be in that class we cannot construct any
20:08
others it follows that we can build a thing that produces the same patterns if we have a system with sufficient
20:14
resources uh well i' I'd like to come back to the simulation hypotheses but or
20:21
hypothesis but for now are you saying that as soon as you became interested in
20:27
artificial intellig it was an interest in generating these simulations or you had more modest aspirations at the
20:34
beginning uh I was never very modest I don't know how that works I also suspect that a lot of uh the humility that
20:42
people have is either meekness or it's an act it's uh I also don't think that
20:48
immodesty is good in any way but uh I was more ignorant than IM modest I was
20:55
just uh not seeing the difficulty I was an interesting question what is the most interesting question that I can work on
21:01
and the most interesting question I can work on is what is reality and what are we what is the mind and so uh this was
21:09
the most interesting question to me and I never really stopped asking why why why and how how how and soon notice that
21:16
the books end right you go through the uh Library shelf and you read all the books that are there on this topic and
21:23
they don't give you the answer they don't explain explain to you how the mind works and you realize oh my God it's an open question and also if we
21:30
actually could construct it we would understand it and I don't see a reason why we cannot construct it except nobody
21:35
got around to yet because we just invented computers and they're still so small and we had just are barely
21:41
learning how to deal with them and to write programs on them that can learn and make sense of reality when you say
21:47
that you're interested in the mind what do you have in mind you're very
21:52
interested in Consciousness or you're interested in the relationship between
21:58
the brain and experience in some sense it was a journey so the first things that I wanted to understand is uh what
22:05
is representation how does representation work and computer science gives strong intuitions on what a
22:11
representation is it's a causal pattern that is implemented on an arbitrary substrate that is representing the
22:18
Dynamics of a domain and allows you to build a system that controls these Dynamics interacts with them and so
22:25
learning would be the building of such representations that allow you to get better at uh dealing with the reality
22:31
around you that so with the pattern around you organize in such a way that you can make predictions of what the
22:36
outcomes of your actions are going to be and then select action paths in them and so also decision- making seems to be
22:42
clear and we can understand motivation from the perspective of control theory and cybernetics so we are a system that
22:49
has hundreds of different needs many of them are physiological a dozen of them maybe are social maybe a handful of them
22:54
are cognitive and then we balance all those needs in homeostatic control system that produces urges to go in a
23:01
certain direction and then uh for a long time the largest question seemed to be emotion to me how is it possible that a
23:09
system feels anything and the answer that I came up with and I wrot my PhD largely about
23:15
this and built models about this together with many students uh was that emotions are configurations of cognition
23:22
so they're not parameters but there are ways in which the system works based on how the environment um and um the
23:29
assessment of the environment happens and as a result we have an unconditional reaction to that reconfiguration and the
23:35
emotion is what this change of what you are and how you relate to yourself and to the world looks from the inside when
23:40
you classify it and so this question seem to be answerable of course there then you have the more details of the
23:46
space of our effects and you can try to analyze this and use this as a framework to start to analyze human psychology and
23:53
map this on existing Frameworks and psychology and see what uh that psychologies have dis discovered many of
23:59
these things before and um organized it in into meaningful framework then there
24:04
also object directed emotions like jealousy or pride and so on where there is something interesting to be
24:10
understood about how an individual need to conceptualize it itself with respect to environment what kind of needs it
24:16
needs to have to be able to have experience violations or anticipations of satisfaction of those needs and then
24:22
experience those as emotions right and then uh now I think the remaining
24:28
important open question is consciousness this seems to be the main question what actually is consciousness and how can we
24:35
implement it and it's really interesting that there is a very large subset of uh
24:41
philosophers and even neuroscientists and so on who don't think that we can possibly understand what Consciousness
24:47
is that is always going to be mystery or it's something that is fundamentally Supernatural and will be outside of the
24:52
pursuit of science even if you are a scientist you're not allowed to say that out loud yeah I think a lot of very
24:57
famous uh neuroscientists psychologists and philosophers think that I mean Steven Pinker is the main name that
25:03
comes to mind as somebody who's who has this mysterian position but the
25:09
mysterian position I think was originally coined by Colin mcin MH um but uh also chsky is a mysterian and I
25:16
think in in a practical interesting yeah I think practically mysterianism is the position that something cannot be
25:22
understood if it cannot be understood by no chsky no no chsky and when I spoke
25:27
with him on the show a long time ago one of the interesting things that he said was the biggest mistake philosophers
25:35
ever made was to treat everything above the neck as different from everything
25:41
below it and that statement because we don't have any in principle difficulties
25:47
with understanding the way that a human knee works or the heart Works something like that it led me to believe that he
25:53
felt Consciousness was something that could in principle be understood we just had to take
25:58
a very um physicalist attitude toward it how would that work I don't know I
26:06
mean that's what the NCC program is but some people don't think that that's very successful the name of the NCC program
26:13
the neural correlates of Consciousness not right it doesn't see the neural causes of Consciousness MH to have a
Does Roger Penrose’s Theory of Consciousness Make Sense?
26:20
causal model of Consciousness that is of course the aspiration and one of the difficulty of half of the theories that
26:26
are Advanced and philosophy or adjacent fields to um explain Consciousness do
26:33
not have any causal structure right so for instance I don't see how penro or um
26:39
orchestrated collapse is actually leading to the phenomenology of Consciousness and how that would be
26:45
related for our listeners who don't know that theory what is it maybe it was not a good idea to open this particular bag
26:51
of worms but um um Roger Penrose is famous for um having um made his
26:58
contributions to understanding black holes he got a Nobel Prize for this in 1967 and he's also written a very
27:06
beautiful U um popular science book called Road to Reality uh that is
27:12
explaining many of the main concepts of mainstream physics at in beautiful level of detail so it's basically a book that
27:19
you can as an intelligent person read without knowing a lot of mathematics and theories of physics and it's um it's not
27:26
that large it's 200 Pages like lot of the Rings or so and it really walks you
27:32
through the main ideas main intuitions of physics and like fan it doesn't let you do exercises so maybe it doesn't
27:38
stick that much but you get very good intuitions about what physicists are talking about and uh he also got um has
27:45
made some contributions called Twisters to understanding of quantum mechanics that are not that much in widespread use
27:52
and then he has developed often in mentioned in conjunction with Stuart hamov um theory that uh Consciousness
28:00
cannot be understood computationally and he's written a book The Emperor's New mind that is um trying
28:06
to elucidate the fact that um mainstream uh cognitive science and Neuroscience
28:13
and so on might be barking up the wrong tree have the wrong ideas on how Consciousness works because ultimately
28:18
their ideas are all computational and he takes girle to prove that uh there are
28:24
limitations to what can be done in formal languages mhm and that these limitations do not apply to human
28:30
Minds which is I think a very bold conjecture and a lot of um there's a lot of philosophy about this yes a lot of
28:37
analytical philosophers I think would um talk back to him and for some reason he
28:42
doesn't feel compelled to listen to The Counter arguments mhm uh and instead he is um arguing well I think that um
28:50
current physics is in some sense computational it basically describes the universe as a state vector and then
28:56
transition function comes along and you get another state Vector right this is a computational model regardless of
29:02
whether it's Quantum or not and so Consciousness must happen in the areas
29:08
that are not in the known physics and the part of um current formal models of physics that are not well understood or
29:14
instance the realm especially in the realm of quantum gravity so maybe it's there MH and so uh penus has a theory
29:21
that I don't actually understand about how or that Quantum collapse leads
29:29
the N Tu part is I think hamov Edition I don't think that they're actually the same Theory because uh you don't need
29:36
extensions to quantum gravity to explain how uh hammer off's idea on how
29:42
computation would work in super conducting micr tuis works but of course Hammer of theory which is not the same
29:49
thing as um penos even though they're published together um is does not rely on uh extending
29:57
physics beyond the computable even though hamov says that but what he
30:02
basically claims is that superc conducting microtubuli are able to uh
30:07
perform Quantum commutations that are non-local by interacting with each other across cellular
30:12
boundaries and I don't have an opinion about this particular Theory because I don't understand anesthesiology and
30:20
cellular Anatomy to such a degree that I would be able to um judge the probability that hamov is correct or
30:27
incorrect and I observe that uh physicists are very skeptical of the
30:32
notion that high temperature super conductivity could work in biological cells in microtubuli and this way I also
30:40
observed that nobody puts large resources in building computers for microtus so basically all the people who
30:46
are experts in the respective field do not take this Theory very seriously but I I have no idea whether that is because
30:53
he is not onto something or because not I can only judge the parts that I understand this is his claims about
30:59
computation about how to interpret girdle and I'm unconvinced by those parts that I um understand mightly put
31:07
but I don't know if this translates to the rest maybe there is something like a global meub balistic mind and Hammer off
31:14
is able to tune into it using the right anesthetics and uh substances and
31:19
medicines and then uh has some insights who knows when I when I hear quantum
31:25
gravity invoked my hackles just raised because I know that Quantum gravitational effects don't manifest
31:32
until you're down at the plank scale and if we could just learn about quantum gravity by investigating the the mind or
31:39
Consciousness that would be very neat yes something interesting involved in
31:45
this when we zoom out and look at the space of possibilities what Consciousness could be the question is
31:50
at which level is it implemented and at the moment I think that most um people
31:56
in Neuroscience who are willing to make statements about uh Consciousness and I don't think that's even a majority most
32:01
of Neuroscience doesn't even look at the phenomenon uh would say it's an intercellular phenomenon basically it
32:08
happens across cells right so for instance via synaptic excitations and so on that produce activation waves that go
32:15
through the brain and so on and somehow in the patterns of these activations we find the encoding of Consciousness but
32:21
uh who knows maybe it's an phenomenon that requires cell mechanisms inside of
32:26
the cell and works orthogonal to the cellular boundaries right it could be that there is some types of molecules
32:33
across the cells that Implement Consciousness and all this activation firing and so on is an additional
32:38
process that happens uh maybe this whole thing happens at the submolecular level
32:43
even maybe it's a pattern that happens at uh the level where Elementary
32:49
particles are implemented or it happens below the level of Elementary particles it it's difficult to reason about it and
32:56
think about it because we don't have have scientific models that are talking about structure at these domains and so
33:03
we could say it's unscientific to to reason about this or think about this but logically we cannot exclude that
33:09
there is structure at this level and that the structure could in principle Implement Consciousness it just seems to
33:14
be far less likely given the phenomenology of Consciousness that we observe so from my perspective what we
33:20
seem to be observing about Consciousness and I might be wrong is it's slow so it
33:26
seems to be com uh compatible with the speed of sound speed in which our brain roughly operates and we look about how
33:32
activation spreads in the brain and uh it seems to be relatively coarse and
33:37
noisy and it seems to depend extremely on the state of my brain right so basically if you squish my brain my
33:44
Consciousness instantly changes if I squish anything else in the universe or any other level uh then um my
33:51
Consciousness doesn't change in the same way when you say that it's coarse is this again going back to the superv of
33:58
mental state oh I also mean that it's it's noisy so it's basically feels as if
34:04
my Consciousness uh does not have an infinite resolution compared to the number of cells or molecules that exist
34:11
in my brain or the number of synopses so it it appears to me that um my Consciousness is able to produce the
34:18
impression of infinite detail in a similar way as a fractal can but the Fidelity of my Consciousness is
34:24
relatively low so the am of pctures that I can reliably track they differences that I can tell between different
34:29
objects and so on is is not that high are there so I take it you are you're
34:35
resolutely not a mysterian no right so I I I also don't think that mysterianism
34:41
makes a lot of sense because I believe that uh whatever the problem is ultimately you're looking at the
34:47
sequence of states and you can always uh come up with a computational model that explains explains an arbitrary State
34:53
sequence it might be extremely convoluted it might be wrong and so on but it's never the case that we cannot find
34:59
an explanation it's difficulty is to find a a useful explanation one that is relevant one that is practical and one
35:07
especially that is truthful in a sense that it actually captures the phenomenology or the features of the
35:13
thing that we want to describe just to get some philosophical cards out on the table do you come with come at this
Could Dualism Explain the Human Mind?
35:22
problem open to dualism or pan psychism or are you more of a physicalist or how
35:29
do you what sort of constrains the possible theories of Consciousness that you find compelling basically I'm open
35:36
to any kind of theory as long as I get it to work okay and uh difficulty with most of the theories that you listed is
35:43
that I don't know how to get them to work and even physicalism for a lot of
35:49
people there seems to be um a belief in physical objects or physical substances involved but uh is this a necessary
35:56
belief for understanding the the mind I I don't know for all I know it's possible that we are always living in a
36:02
dream and in some sense I find myself to be living in a dream right it's that's
36:08
my experience the objects that I experience including the device that makes physical measurements are objects
36:16
that I dream and this dream the best explanation for the regularity in the dream is that the dream is
36:22
constructed U to track regularities in sensory data and it's constructed inside
36:28
of the brain of a primate mhm right but this brain of the primate and and so on how do I know about the brain of the
36:34
primate oh my teacher told me how does my teacher Tau me they learned at University and read it in books and so
36:41
on and so on right but who came up with this idea and based on what and what were the available alternatives to these
36:47
ideas that's ultimately what what we need to ask and basically how leaky is the epistemic chain from uh the
36:55
observations uh over the arguments and conclusions and the theory space to me when my teacher tells me and as a child
37:02
I was extremely skeptical about the coherence of that chain because first of all I was super U nerdy and ignorant and
37:10
stubborn and second I lived in communist Eastern Germany so per default everything that my teachers uh told me
37:16
was wrong so from my own perspective also came from an artist family that didn't
37:21
believe very much in the state propaganda and they had tons of books in my house and I realized that many of the
37:27
people people who had written books hundreds of years ago I had more in common with them than with my teachers
37:32
uh because they were more rigorous in their thinking and they actually care about thoughts that I had and in a way in which my teachers didn't and so I
37:39
found very often that of course the teachers are giving us models that are tested that are useful in practice that
37:45
have worked in practice but if I ask them metaphysical questions they usually not even interested and when they give
37:51
answers these answers tend to be shallow or wrong and so if you have theories like physicalism what does it actually
37:56
mean and to me right now physicalism means that there is a closed cly closed
38:03
mechanical layer to reality and be directly supervene on it right it's uh
38:10
if you it could be that there is a physical universe that is entirely mechanical and in this physical Universe
38:15
exists the computer that is operated by some nerd and you exist as a simulacrum in this computer and the world around
38:21
you is arbitrary right but that means they not in a physicalist world for instance if you live in Minecraft uh
38:27
even if Minecraft exists on a physical computer uh this would not be physicalism because you would not
38:33
directly supervening on this mechanical layer the regularities That You observe are only the result of arbitrary ideas
38:39
of a programmer not the result of what's happening in physics at the lowest level right so physicalism is this idea we
38:46
actually in base reality and if I think about what's an alternative to physicalism beyond the
38:52
simulation Theory beyond the idea that there is this cly closed mechanical layer is some else um how does this
39:00
work what would it look like if there was no Foundation to the world or or is
39:06
it just a conspiracy I just don't know how to think about this and uh so I can think
39:13
about the idea that the Universe maybe at the lowest level is some kind of topology that explains why there is
39:18
something rather than nothing and then I I see at branching Multiverse by
39:24
applying all possible operators simultaneously and so on I come to a possible explanation that I am in a in a
39:30
branch of this space of all possible realities that would exist as such a mathematical topology right and this
39:37
ultimately is a physicalist worldview but it's quite different from what a school teacher would call
39:43
physicalism right mhm and also to understand physicalism correctly we still need to accept the fact that what
39:51
you're looking at is not rest extensa in the cartisian sense because Bas deart
39:56
thought that uh what we direct here when we touch stuff and we U are observing
40:02
ourselves to be in space and so on is in some sense the physical universe but of course it's not it's a game engine
40:08
generated by your brain there are no colors or sounds and so on in physics they only exist in your own mind so I do
40:14
have to accept that the dreams at night are not that different from the dreams during the day only the dreams during
40:19
the day are much much more stable because they're tuned to model sensory data that are kept stable by some
40:25
pattern generator outside of my mind which we call the physical Universe right so in this sense I could you could
40:32
say that I'm a dualist because I accept that there is a representational layer in which I exist as a perceiving being
40:39
as a cal structure that is ultimately Software State transitions uh implemented on a different substrate and
40:45
ignoring the actual physical Dynamics beyond the fact that they can be used for representation and then there is
40:50
this physical universe that makes all of these things possible but I can never visit this physical Universe because I'm not conscious in physics and only
40:57
conscious in my own dream of reality in my mind in my representations when you
What’s Wrong with Panpsychism?
41:04
say that what constrains your theory choice is whether or not this you can
41:09
get this Theory to work I'm wondering whether you mean let's just take pan
41:15
psychism for example you're saying that to for pan psychism to work means that
41:22
you can tell a story that you find compelling and explanatory for how it
41:28
accounts for human consciousness or you're saying that and this gets us back
41:33
to computer science you don't know how to use pan psychism to program an AGI
41:40
that is conscious so um how would you formalize psychism and you actually spell it out
41:46
what is the relationship between observables the structure that realizes
41:52
the observables and uh for instance the phenomenology of consciousness uh that
41:57
you want to explain how do they relate to each other and I find that a lot of
42:03
pists are unable to spell this out in a way that is satisfying that basically
42:09
makes sense or spell it out in a way that makes it different from say computational dysfunctionalism and maybe
42:16
take a step back and explain what what computational D functionalism means right so first of all functionalism is
42:21
an epistemological position epistemology means uh our Theory on what can we know
42:28
about reality and how do we access to what we know and uh in functionalism the
42:33
idea is that we construct objects based on Behavior so for instance we construct
42:40
the idea of a water molecule based on the behavior of a certain aspect of physical reality that is in some sense
42:46
invariant right so we notice that we can split the water molecule we notice that uh we can combine many motor molecules
42:54
into liquid that is water and we can explain Water by being a combination of these molecules at certain temperatures
42:59
and so on right and so this is all Behavior there is never do we get an
43:05
contact with the essence of a water molecule with something that is beyond observable Behavior so everything that
43:11
we construct as a water molecule is the result of us trying to regularize and
43:16
Abstract all our observations related to that phenomenon right and so the idea
43:21
that it's explained explainable by this particular singular phenomenon and so on um makes sense from the perspective of
43:29
functionalism because the water molecule is describing a set of functions that can be uh that lead to interactions with
43:37
other changes in other information in the world right it would also not make sense from this perspective to say that
43:43
there is an object in physics that is behaving in every single way like a water molecule that shares exactly all
43:49
the properties of a water molecule in every Regard in every possible measurement that can be made but it's
43:55
different from a water molecule right because we never got to the idea of a water molecule from from something
44:01
else but behavior that we observe right right and so uh there is this idea the
44:07
alternative to functionalism would be some kind of essentialism that there is some hidden Essence that yet makes
44:14
itself known to us in some way Beyond behavior and I don't know how that would work right otherwise if it's observable
44:20
Behavior again it would be a property that we observed that would be translated in some function and the
44:25
other one is computation ISM the other aspect and that's a position about representation it basically says that we
44:31
can represent an arbitrary thing or model an arbitrary thing using State Transitions and to say that there are
44:38
things that cannot be modeled by state transitions is doesn't sound absurd but it's a very bold statement because it
44:44
means that uh physicists are missing something or everybody is missing something who is making models so what
44:50
should that be and so uh I think that this position of computationalist functionalism is relatively basic
44:57
idea about how representation works and how epistemology Works how objects are constructed and knowledge is constructed
45:04
over those objects and uh so when we are trying to attack this position and
45:09
somebody says I'm going to build a non-functional theory of Consciousness I am very curious and I'm for instance
45:17
Julia tononi is trying to do this in IIT but IIT is trying to describe uh Consciousness using information
45:24
theoretical regularities which means functions and this is difficult right because at
45:30
which point is he able to introduce something in the theory that is not functionalist and yet is somehow a
45:36
notion that you can work with as a philosopher or as a scientist and so I
45:42
it seems to me um this belief in Essences that do not reveal themselves yet we know about them seems to be a
45:48
Superstition it seems to build on some kind of intuition that I construct in my mind but when I closely examine the
45:54
intuition I notice that it's empty that there is no way in which I can build
45:59
something that is realizing the uh some kind of formal difference between that intuition and another system so when
46:06
somebody is saying there a penist and they say that for instance Consciousness is an aspect of matter that is
46:12
distributed in the universe and it's everywhere in the universe right you would probably uh if you ask this person
46:19
does this mean that everything in the universe is equally conscious uh it's probably not what you
46:24
want to get out of this right there is seems to be some kind of difference in the phonology between a person that is
46:30
anesthesized and a person that is not or a person that is dead and a person that is not or the empty space between people
46:36
MH right so even if there is something going on in the empty space between people it's to some degree different
46:43
than what's happening inside of your organism especially your brain with respect to Consciousness so you have to
46:49
explain this somehow right so but if you have a form of psychism where you say okay but it depends on the structural
46:55
properties of the matter and only when you have a certain density of certain structural properties and so on then uh
47:01
you could say that um what is the seat of software in the world and it's everywhere in the world and but it's
47:06
implemented to a different degree depending on the uh functional structure of things that are enacting it right and
47:13
so the more you are uh looking into this as soon as you then identify criteria for when the structure exists like uh
47:20
Control Systems energy consumption and so on stability of a substrate and so on
47:25
the more you add to this the more your peny theory turns into box standard functionalism right and unless you uh
47:32
make a radical step and say all your observations have actually nothing to do with Consciousness the fact that you are
47:38
behaving differently when you get anesthesized or when your brain gets destroyed is completely irrelevant to
47:45
Consciousness because Consciousness is something that's much more fundamental than this right uh but then we are not
47:50
talking about the same phenomenon anymore we are talking about your idea of something fundamental that can no
47:55
longer be mapped into my into what I observe and what I try to make sense of are you looking toward philosophers or
48:04
neuroscientists or computer scientists or some other class of researcher to
48:10
make the next big breakthroughs in Consciousness or is it all three of them because I mean you don't think
48:17
philosophers are doing good work on representation for instance so I'm sure that philosophers are good doing good
48:22
work on representation there are extremely smart philosophers who do analytic philosophy of the highest
48:29
regard it's just when you look at the mainstream of philosophy as a field and you look at what goes into the Canon of
48:35
philosophy at the moment I feel that uh even um GLE has not been digested and so
48:41
when you have a person that is educated in philosophy and has uh read the
48:47
mainstream literature and has a degree in philosophy and so on and you ask them to summarize girdles proof there are
48:53
probably not adequate compared to a computer scientist who has a good gotten a good um education and formal logic and
49:01
I'm somewhat flabbergasted by this that you have um summaries of goodles poof
49:06
even books about it that still treat it as some kind of sculpture in the museum
49:11
of mathematics that is somehow very important and very grave but it does not actually change the way in which we
49:17
think about reality and it should if you understand it who though is it that you're looking to for inspiration or
49:25
further developing theories of Consciousness there is a very large number of individuals in in many fields
49:30
that do this so it's not I don't think that there are Fields uh where there's no single smart uh generalist thinker
49:39
but I think that at the moment people that I find very inspiring for instance mlav I saw you spoke with him recently
49:46
uh here's somebody who is saying a version of what your attributed to tomsky uh in in the sense that there is
49:55
a big similarity between how development Works an organism and how representation and organization Works within a mind and
50:02
maybe this is mostly a difference of uh time scale and not so much uh just a
50:08
similarity that is superficial um so maybe these principles of self organization that we observe in
50:14
organisms are uh crucial for understanding the self organization in the mind it could also be that when we
50:20
zoom out very far that we find many of these principles reflected in the organization of societies and
50:26
organization of course uh I don't think it makes sense to claim that societies are
50:31
conscious in the way in which people are conscious for similar reason I don't think it it makes sense to assume a
50:39
priority that organisms by themselves are conscious in the same way as um nervous systems produce Consciousness if
50:46
they do right uh but um H in neoscience I think the number of
50:53
people who uh work on this question of Consciousness to to some degree for instance one of the most famous teams is
50:59
run by Stanis left hin and there many people who are making important contributions in this feic and Antonio
51:05
damasio um Michael graciano has uh has had an influential uh set of ideas in
51:12
this space attention uh yes this attention schema Theory mhm um but on the other hand you have philosophers
51:18
like Thomas metzinger and so on um David Charmers who are very smart who think deeply about these questions but
51:25
ultimately I think to make progress we need to move to testable theories and
51:30
most Fields do not have very uh good methodological Frameworks right now to
51:35
to fault of their own so one of the reasons why psychology ironically has started to stopped studying the psy is
51:43
uh because psychology understands in some sense that the psy is a system with so many variables that you will not get
51:49
a model where you can test your theories in a lab using the existing statistical
51:54
methods so if and you have not testable theories it's not science and so psychology is
52:02
makes the decision to be a science instead of being a practice uh that or a
52:07
literature form or something like this or in some sense I think that uh it has
52:14
excluded Philosophy from its own Pursuit which maybe it shouldn't so if you for instance look at PR I don't think that P
52:21
is is so much unscientific in philosophy of William James in psychology it's that
52:27
they are interdisplinary between psychology as a science and the
52:32
philosophy of mind that is necessary to understand human development human representation and so on so in a way I
52:38
think these fields would have to be bolder and do more philosophy and philosophy might also have to be bolder
52:44
to go back into actually describing phenomena that we observe and cannot yet yet explain rather than writing about
52:51
what other philosophers said about it toward connecting Consciousness with we've been
52:56
talking about and then artificial intelligence maybe first we just ought to get on the table a definition of
What Is Intelligence?
53:04
intelligence for getting the artificial part but how do you think about what intelligence is at the moment I think of
53:12
intelligence is the ability to make models okay right so you could say it's uh what you use when you don't know what
53:19
to do intelligence is not just deploying a skill or having a skill right this is um
53:25
maybe this is complet unrelated to intelligence because I would argue that a chess computer that is merely enacting
53:31
and pre defined algorithm is by itself not intelligent even if it's performing
53:36
a task for which people would need to be intelligent to perform it but uh this ability to make a new model usually in
53:43
the service of control is intelligence and what I like about this uh approach
53:48
to defining it is that it doesn't say anything about performance or success I I think we observe that some
53:56
people can be highly intelligent without being wise or smart right right and so wisdom uh or smarts are slightly
54:03
different from intelligence they have to do something with go rationality and so on but some people basically compensate
54:09
their regulation deficits or the lack of wisdom by being more intelligent by making more models and these models can
54:15
be extremely detailed and uh they can be very very clever and so on without
54:21
actually being helpful right so intelligence is a certain faculty that you deploy in a certain area of the
54:26
cognitive function it's not the same thing as agency and so for instance when you are a CEO of a company what you
54:33
ought to maximize is probably not your intelligence your ability to make models but your ability to actually solve
54:39
problems in the pursuit of the goals that you have and so you should deploy intelligence where it's necessary but
54:44
you should also not uh spend all the time solving problems just because problems exist right and many uh highly
54:51
intelligent people really do enjoy specializing on on that kind of pursuit right it means that they play an
54:57
important role in an organization like a company or uh in a research team but uh
55:02
the ability to make models is by itself not the most important thing when you want to be successful in the universe
55:08
mhm so I I would make a difference between intelligence and agency the agency is the ability to control the
55:14
future so intelligence in its relationship to making models is very
55:21
closely associated with a capacity to solve problems but it's not solving
55:27
those problems because that requires deploying the intelligence yes requires action and uh it also um requires making
55:35
the right models evaluating them in a particular way and so on MH but uh intelligence is uh in some sense you
55:41
could say in acut test what we measure is the ability of people to solve puzzles how does artificial intelligence
55:48
differ from just intelligence artificial intelligence is in some sense a slogan and it is a field
55:55
that was started by Marvin Minsky and John McCarthy and a few others uh in recognition that the computers are
56:02
offering a new paradigm it has happened in the 1950s and uh Marin Minsky
56:08
positioned artificial intelligence as an alternative and extension to cybernetics
56:13
many of the original uh founders of the field and people who joined it were cyberneticians and cybernetics was the
56:19
idea that we can describe uh our ability to make models and ultimately our minds using control theory using feedback
56:26
loops and so on and uh computation is generalizing this in a way it goes away from this continuous notion of uh
56:33
dynamics that you control to automata that you can from the ground of construct to do anything describe
56:40
anything including those feedback systems and these control systems and you could say that initially there was
56:46
some kind of uh rivalry between uh cybernetics approaches and the computer
56:52
science approaches the AI approaches because a classical AI is mostly looking at symbolic systems at discrete systems
56:58
at ways how to combine those operators to build the structures that you want
57:03
and instead of going straight for the control thing and so there is the question what is more fundamental is at
57:09
the fundamental level what should what you should be starting with is this the continuous control stuff the dynamical
57:15
systems or is it the world of the automata and we can show that there in some sense equivalent you can build
57:21
automata from control structure and arguably that's what we doing when we building transistors you just make them
57:27
so big that the control structure is stable enough to produce these multi-stable transistors and then on top of this we
57:34
build discrete logic that is allowing to us to do arithmetic and then line
57:39
algebra and then we build neural networks that are continuous again and then we train the neural networks to speak discrete languages again and so on
57:46
right so you can build layer on layer and you alternate between the discrete logical stuff and the continuous control
57:52
stuff right that is more geometric but uh Minsky um opposed thetics in a way
57:59
and I think that uh because he thought that this um computational Auto automat
58:04
approach is more fundamental he actively opposed both neural networks and cybernetics and led to a rift in
58:11
computer science and I think this was with hindsight uh not a very good Rift
58:17
because it led to a delay of connecting ideas across Fields now I'm wondering
What Defines the Current Wave of AI?
58:23
one of the things that you wanted to speak about today that I'm very curious to hear about is the current wave of AI
58:32
and what what it is and and where it's heading so it's difficult to say what the current wave of air actually is
58:39
right because there are many ways to look at it is it deep learning or is it the Transformer or is it the llm right
58:44
so with different degrees of specificity but you could say that the um in some sense the Deep learning
58:51
Revolution started um in the um 2000s
58:57
when uh people discovered that it's possible to train newal networks with almost arbitrary numbers of layers mhm
59:04
and almost arbitrary numbers of parameters and you just need to use more data and more compute to do that and
59:10
then people came up with uh some algorithms that made this efficient so it was actually feasible for large
59:17
amount of data like training on the entire internet or training on hundreds of millions of pictures and uh the first
59:25
algorithm that was discovered in this regard was the Transformer and the Transformer was uh this breakthrough
59:31
after the Deep learning Revolution that discovered that we can selective basic
59:36
make selections over what we should be learning basically make Statistics over what we need to make the statistics over
59:42
it was one of the aspects of the Transformer that you basically train your new will Network what to pay attention in the previous layer too so
59:48
you don't look at everything at the same time you learn in the present context what things are matter most and the
59:55
other aspect of the Transformer that was super important was to make this parallelizable so you can run this on a
1:00:00
distributor GPU farm and make it feasible to do it fast because if you want to make it faster you can distribute it on more CPUs uh gpus
1:00:07
instead of just waiting for one to compute longer right and so uh the trick to training on the entire internet all
1:00:15
the text in the internet is to to paralyze this as well and uh this is
1:00:22
what's driving most of the present progress and most people agree that is not the
1:00:28
answer but it is an important part of the answer and the scaling hypothesis basically is the idea that the present
1:00:34
classes of algorithm together with twists and improvements and so on and some important insights that we're going
1:00:40
to make along the way are going to be sufficient to get us to artificial
1:00:46
general intelligence AGI is a notion that was came up after the first AI
1:00:51
winter in way because U the first AI what AI WI
1:00:57
um so what happened is that in the 50s when Minsky came up with this idea of teaching computers how to think he
1:01:03
thought this is uh going to be uh not it's not going to take us very long we
1:01:08
probably going to be able to figure out what this is the generalization of thought and learning and implement this
1:01:14
in a computer and then we just need to wait for the computers to be big enough and everything will fall into place and
1:01:19
at some point he say it's going to take between 4 and 400 years who knows oh wow so uh that at this point he already took
1:01:26
a step back and then in the 1970s um 1972 uh a team in edenburg um was trying
1:01:33
to get new funding for a lab that was combining ideas and Ambitions from Robotics and uh learning and AI into an
1:01:43
AI um project and the Royal Society put an outsider on this to study this to see
1:01:50
whether this basally a general list This was um sir uh lill lill was an not only
1:01:57
an atmospheric scientist but he was a polyas and very smart and he didn't get along with uh Denis mitchi and uh so he
1:02:05
basically he recommended in U um a review that was leaked that uh this was
1:02:14
not to be funded and his argument was mainly it makes sense to build computer robots it also makes sense to uh build
1:02:22
um models of the human brain um to be more brain like but uh to a model for
1:02:28
neuron functioning and so on but it doesn't make sense to try to build robots that are humanlike or that that
1:02:34
basically have a humanik mind and they try the original AI researchers and John McCarthy and others tried to explain
1:02:40
this is not the point it's not about the robots it's about building Minds it's about Building Systems that can learn
1:02:45
generalize abstract and so on and we think that is possible and doesn't necessarily require that we do exactly
1:02:51
what biological brains are doing I try to mimic this or try to understand neurons or NE chemistry but it requires
1:02:57
that we understand what the brain is doing what it's Computing the mathematical principle behind it and uh
1:03:04
I think there is a big cultural opposition in our society our society is and I think this might be unique feature
1:03:11
of Western Christian and post-christian Society dualist in a sense there are the
1:03:17
world that we are in is some kind of simulation some kind of Minecraft and our souls are sitting outside of this
1:03:23
world the true mind is not of this world it's some something Supernatural that is in the layer outside and above natural
1:03:31
reality natural mechanical physics and somehow talks to it and interacts with it and the idea that we could be NPCs
1:03:37
that we arejust patterns within the physical world patterns within Minecraft is Blasphemous it's an insult to how we
1:03:44
perceive ourselves and uh I think this is part behind this opposition and this
1:03:50
delal report led to a counseling of funding for most AI research oh wow and
1:03:56
to this what was called the AI winter and it basically happened uh I was born 1973 so I was born into the first AI
1:04:03
winter and in 1980s we got progress in rule-based system and logic and then
1:04:09
came expert systems and so on and expert systems famousy didn't scale into Ai and in a way it's only deep learning that is
1:04:17
solving this vidin steinan puzzle how is it possible to ground language in reality how is it possible for a system
1:04:24
to build representations that mean something and regardless of how optimistic we are about the mind like
1:04:31
structure or the potential of llms I think it's uh somewhat clear that they
1:04:37
are able to answer the question of what uh what does it mean to mean something because it uh meaning is the reference
1:04:44
to a model to an integrated model of reality and the interesting thing is that the llm is able to build an
1:04:51
integrated model of the entirity of the universe what do you mean by that so when we are talking about meaning what
1:04:57
we we don't refer to something outside of ourselves and there are these uh reference theories of truths in some
1:05:03
areas of philosophy that I don't know how to get to work because I don't know how to make an arrow outside of my
1:05:08
language into a reality right and this is something that mathematicians have
1:05:14
discovered and uh talked about for a long time that Li theorum and so on it's not possible for a system of
1:05:20
representation to prefer outside of itself it's also something that girle recognized so his way to talk about
1:05:27
mathematics would would require to talk about the definition of the language that he uses for his mathematics who is
1:05:34
logic and this definition would be outside right so the way to do this is you need to rebuild the language in
1:05:40
inside of itself to build an emulator and it's brilliant that he discovered this GLE built the first emulator for
1:05:47
making his proof where he is emulating his logical language inside of the logical language and thereby is able to
1:05:52
make references to the inside and I believe the same thing is true for our us for ourselves to be in a mental state
1:05:59
doesn't mean that we know what this means to be in that mental state or even that we are in that mental state we have
1:06:04
to make a model of us being in that mental state right so we have to build a simulation of ourselves inside of
1:06:10
ourselves and the simulation is not functionally equivalent right of course it's a simplification abstraction of
1:06:16
certain features of how we work but uh if we had enough capacity we could
1:06:21
possibly get to the point where we make a model of how we actually work and uh
1:06:27
there are some people which argue if we could make a model of how we work we would not be complicated enough to to do
1:06:33
that that it would be a paradox but I don't think that's the case right it's only a matter of how much memory you
1:06:38
have and how much redundancy you have in the system because of course you do not want to explain every single of your
1:06:44
memories you want to explain how you can form memories and experience them as um
1:06:49
recalls of a past situation and so on and use them in the context of all your mental operations and for that you don't
1:06:55
need to explain all these details you only need to create a few very specific memories right to explain all this
1:07:01
functionality in the same way it should be possible to train an llm on papers about how LMS are written until the llm
1:07:08
is able to not only explain to a human how an llm works but also write the code
1:07:13
for making llm and even possibly modify it and so to get back to this question
1:07:19
what is the what are the limitations and uh possibilities of the current technology the for answer is nobody
1:07:26
really knows some people have strong ideas about it where there people which make strong bets on it that just by
1:07:32
making more training to get models that are more predictive of text they're able to get these models to also reason
1:07:39
better and ultimately to turn language into a selfplay where this thing can reason with itself and derive uh the
1:07:45
remon conjecture or something like that that is not in the training data and uh there are other people who say um maybe
1:07:53
this is going to plateau and you need to look from a completely different angle to get to um an intelligence that is
1:08:00
self-improving and that learns as efficiently as us both sides I think uh
1:08:06
agree in a sense that l&m's are not the way in which the mind works the human Minds works right it's it's only
1:08:12
something that happens to work very well to uh extract structure from textual data this was another thing that Chomsky
Does ChatGPT Mirror the Human Mind?
1:08:20
really stressed when we spoke was that you will he said that you will not learn
1:08:25
about the human mind or how human language works by studying llms so I'm not so sure about the letter
1:08:32
I think that Lum language ultimately is the solution to a particular problem it's the problem is how to translate
1:08:39
mental representations which are some kind of fuzzy hierarchical hypergraph uh
1:08:45
in in such a way that another human being is able to build an equivalent
1:08:50
representation in their mind and to make that happen we need to constrain
1:08:56
uh the mental representations into a protocol that is learnable and to and also interpretable
1:09:03
with very limited resources and our brain has very limited resources because it's not able to perform arbitrary
1:09:09
operations in a stable manner there are certain uh problems that we cannot solve uh in a parallelizable fashion so for
1:09:16
instance when we parse language uh it makes sense to limit the stack size that we need to do that if we right in a
1:09:23
computer you can make a stack that is uh millions of bites deep and that's not a big problem but in a human mind if you
1:09:30
go more and four levels of indirection you lose track of what's happening because you can only keep so many balls
1:09:36
in the air so in some sense human languages are constrained to um mapping metal representations into Strings of
1:09:43
discrete symbols that need to be so few that they are learnable and they need to be uh Parable so inter interpretable and
1:09:50
constructable these strings with a stack depth of say four or so give take and uh
1:09:57
this defines a space of possible solutions and I suspect that human languages are roughly congruent with
1:10:03
that space in a way it's solutions that are given by the constraints of that space and if you want to discover the
1:10:11
criteria that are common to all human languages we can probably evaluate commonalities in uh the tokens of LMS
1:10:19
that have learned multiple languages yes what what chsky was stressing was that
1:10:25
the fact that llms can learn quote unquote impossible languages that humans
1:10:31
cannot learn is just a proof that they
1:10:37
work in such a radically different way from our own language learning
1:10:42
faculties that were just too dissimilar for us to use llms to learn about human language I don't think that one follows
1:10:49
from the other the um premise is of course true the l m training works very
1:10:56
differently from the way in which we learn language we could not learn language in the same way as an llm does
1:11:02
and uh on the other hand the llm is not just more inefficient because we
1:11:07
basically need to give it massive amounts of data before it's able to learn a language but once we train an
1:11:13
llm to do that uh we can also uh give it a dictionary of a new language that was
1:11:18
not in the training data and a grammar and it's able to internalize this and then instantly use that language at the
1:11:25
level of a human speaker and that's something that a human speaker couldn't not do because a human speaker would
1:11:31
need some more practice arguably to memorize all those words and uh extrapolate the grammar and understand
1:11:37
it right so the stuff that's falling in place in the llm because it has vastly more resources and very much more
1:11:45
powerful way to represent C the structure than the human brain does um that is not available to the human brain
1:11:51
so the llm is both more and less efficient than the human brain is but the LM is building uh a representational
1:11:58
structure and this representational structure is aligned to language use and
1:12:04
it would be surprising if there are not some some similarities in the structure that the llm is discovering and in the
1:12:10
structure that the brain is discovering at a certain level of course graining at a certain level of abstraction and we
1:12:16
find this with vision for instance when we are training an endtoend system on uh
1:12:21
images to be able to reproduce images we find that the hierarchy of this Vision model is quite similar to the hierarchy
1:12:28
of the visual models in the human brain so we find equivalence of a lot of the
1:12:36
brain areas basically in the layers of uh the neural network despite the neural
1:12:42
network not working oneto one comparable to neurons but uh this is something that
1:12:48
Chris Ola calls the universality hypothesis which is basically if you have a general enough representational
1:12:54
substrate and uh you train it uh sufficiently long on a given problem then uh different um
1:13:02
modeling approaches in some sense are going to lead to a similar model structure given this model
1:13:09
generating definition or understanding of intelligence before we move on to
1:13:14
some of the possibilities and limitations that you alluded to earlier I'm curious how you think that some of
1:13:23
the major Technologies in the current AI wave such as chat GPT how well they or
1:13:31
how much they qualify as intelligent it seems that the U models are able to um
1:13:37
be intelligent at a certain time scale and certain contexts where humans are not intelligent and others where they
1:13:44
are and one of the issues with the present approaches is that the systems that we displ deploy uh they make
1:13:53
basically they H hallucinate and it's also arguably a property of our human mind that we hallucinate I suspect
1:14:00
that I have a basic hallucination ability in my brain that is always on but I have a critic on the other side
1:14:06
that is uh cutting down these hallucinations and for instance um test
1:14:11
them against sensory data and when the hallucinations of my visual system are not predicting my visual data very well
1:14:17
they basically get pruned and only those that are uh predictive are continued but
1:14:23
when I dissociate from my sensory inputs for instance at night when I sleep I
1:14:29
don't have sensory input anymore but these uh visual uh modeling area is still active it's going to produce uh
1:14:37
continuations of its present State and these uh play out as dreams right so
1:14:42
they're basically free running hallucinations and in many ways you could argue that what the llm is
1:14:47
producing when it just continuous text it does something very similar and uh people find this very problematic
1:14:53
because when you train a model on the internet and you don't train it on being uh always factually correct but you
1:15:00
train it on producing text like on the internet it can give you Wikipedia article about non-existing people right
1:15:07
and so uh that's a big problem when you are trying to use it in a context where you try to replace Wikipedia with your
1:15:13
llm instead of getting ideas for writing a new Wikipedia article for a new non-existing topic right and so maybe
1:15:21
this is a task where we have to communicate clearly that the llm is actually not comparable to human brain
1:15:27
and its ability to reason and give you truthful information but it's somewhat comparable uh to a human brain and the
1:15:34
ability to come up with new ideas but it cannot test these ideas because it's it
1:15:39
is doing this mostly in a one short form in the same way as you are producing one Association and without rehearsing this
1:15:46
Association spelling it out as text right normally when you are trying to solve a problem or develop a
1:15:51
philosophical idea you're not going to use that intuition as your final output you're going to go back and criticize
1:15:57
this intuition look for all the loopholes that you've seen and so on and prune it and right and get it down to
1:16:03
something that works and it's an iterative process that doesn't go once but it's a back and force and so this is
1:16:09
missing in these present approaches and a lot of people in the AI companies which work on building that and in the
1:16:16
meantime what they're doing is they fine tune these models to be less creative so
1:16:21
they um P their output pre tively by training them on uh stuff that is um
1:16:29
basically proven or reliable and they penalize them for producing stuff that looks unreliable and as a result they
1:16:35
tend to produce more stuff that looks more reliable and more authoritative which arguably I think is even more
1:16:42
problematic because now it looks as if it was authoritative but it is uh now basically just emulating a person who
1:16:50
understands so you really can't tell if it's a Wikipedia article a fake Wikipedia yeah or uh an opinion piece by
1:16:58
a journalist who just has an opinion and is only socially motivated right so it's basically it's it's now this some people
1:17:04
disparage social sciences as the soft Sciences because you get away with saying stuff that doesn't need to
1:17:10
replicate because the criteria for whether this is publishable are depending on your community on the
1:17:16
social Acclaim that you get by publishing it right so there is no hard and fast analytical Criterion or
1:17:21
empirical Criterion that determines whether what you are saying is true or not it's entirely a social Criterion in
1:17:27
a way the llm is uh when you don't mitigate it operating like this and if
1:17:33
you make it sound more authoritative uh it's infuriating to try to get this
1:17:38
thing to be creative again because now it is both wrong and uh stubborn in
1:17:44
insisting on not being creative right so that for me is a big issue especially
1:17:50
when you for instance take an llm and you ask it an interesting philosophical question that philosophers don't have a
1:17:57
consensus answer for and I thing is trying to emulate a non-existing consensus something that I found
1:18:03
refreshing at the outset of your response was that you said it is intelligent in some context but not
1:18:10
intelligent in others and what I appreciate about that is I think people have a pretty major anthropocentric bias
1:18:19
and we don't want to label computers or nonhumans as intelligent so for instance
1:18:26
I mean there might once have been this idea that if a a computer has to be
1:18:33
intelligent if it can beat a human at chess but as soon as the computer beats the human at chess we shift the goalpost
1:18:39
so I just appreciate that you don't see it that way but the next thing that I'm wondering about is whether or not there
Will ChatGPT Become Smarter than Humans?
1:18:47
is a simple answer to what has to be done to get the current generation of AI
1:18:55
from being intelligent in certain contexts to being intelligent in a much
1:19:02
broader context so that they're generally intelligent and some sense it's ironic that people thought that uh
1:19:09
logic is going to be easy but common sense is going to be hard and it turns out that the llm in some sense is
1:19:15
solving the problem of common sense but it is has difficulty with Logic on the
1:19:21
other hand these systems are getting better right now we have systems that are able to to solve tasks at the level of maass
1:19:27
Olympics but I still find that in many contexts when I use the llm uh in the
1:19:32
areas where I'm actually competent it does not necessarily reach my level of competence and so there is this question
1:19:39
is the llm that is imitating people in all Dimensions going to surpass people
1:19:44
by imitating them on more Dimensions that an individual can be competent at right so an areas where I have no competence the LM is usually better than
1:19:51
me already right and that uh that is really interesting this especially for instance for people who cannot write or
1:19:57
people who are not able to understand complicated text the llm might not be good at comp understanding complicated
1:20:03
text as as good as a lawyer who is competent in this field or as a scientist or as a philosopher but it's
1:20:09
going to be much better than a person who doesn't understand how to read that kind of text or to write it this is a really nice Point yeah but it's also
1:20:16
elucidates some of the danger right because you might come to rely on a system that uh is giving you some kind
1:20:23
of fake competence that that looks as if it was competent but you don't because
1:20:28
you are unable to fully understand its limitations because you're not proficient yourself you don't really
1:20:34
understand where it doesn't work anymore and for me a good Benchmark is still writing programming code in an area that
1:20:41
that you understand and think about is this thing able to outperform you in writing novel algorithms for instance
1:20:47
and uh how many mistakes does it make and how logical are these mistakes of course it's very good at writing boiler
1:20:53
plate and solving problems that have been discussed on stack Overflow but it's uh I still find that coherently
1:21:00
solving complicated problems and expressing them as code is still something that uh even the leading llms
1:21:05
that are publicly available cannot solve at a human level but uh if you are such
1:21:10
a programmer who can do that then that thing can make you more productive is there a known path though for getting
1:21:18
the llms to be able to write code that is more competent than yours in an area
1:21:24
in which you're already pretty competent that's an open question okay but it would be surprising if there was
1:21:31
no path right so uh of course there are people who are making bets on on that
1:21:36
and what are some of the options with work in teams on on this question and so
1:21:42
how can we make more long form reasoning one way of doing that would be to try to
1:21:47
turn programming into a selfplay task for instance AI um systems get much better uh than people at playing go just
1:21:55
by playing against themselves and because they are so fast and if you give them enough resources they're able to
1:22:01
outperform thousands of years of human go play visin a day right so if you say
1:22:06
that a human go player is of course not learning the entire game of goal by themselves if they at the expert level
1:22:11
but they are uh um drawing on the experience of goal players in the past and the similar thing is for Grand
1:22:18
Masters in chess use uh the insights of chess players before them to get better at chess this is all not NE necessary
1:22:24
for the llm because not for the LM but for the AI training system because it's able to uh surpass that performance of
1:22:32
human games by playing against itself and finding regularities in these games
1:22:38
and so there is this question how can we turn language and logic and so on into selfplay and that would require that we
1:22:45
are grounding the logic that we're using to describe a world onto first principles and understand how we get
1:22:51
from those first principles to The Domain that we interested in and maybe
1:22:57
also uh get an intuition about the first principles in in the reality around us
1:23:02
when we look at the great minds of physicists people like Tesla or Marone
1:23:08
and so on people who for instance build the first uh radio uh I asked my teachers how did
1:23:15
they get to this first radio how how did they understand this and then you learn
1:23:20
yeah there's this idea of electromagnetism and how it propagates in the air and so on yeah sure but how
1:23:26
do you get to this idea how do you have this idea that you put a rod on the ground and this serves as a feedback
1:23:32
over and basically almost arbitrary distance and the other one goes through the air and you modulate in this way and
1:23:38
build an oscillator why would you think that this works in the first place right and uh in the absence of spelled out
1:23:44
theories and general available knowledge about these regularities there are people which
1:23:49
build these intuitions about how reality works at a deeper level and arguably that's what we are
1:23:55
doing all the time that we have these deep intuitions about how to interact in Social domains how to interact in
1:24:01
Practical domains how to interact with even geometry when we trying to make a mathematical proof and we often not
1:24:06
aware of those intuitions and we can get a bunch of those intuitions into the llms but the llms at the moment are for
1:24:14
instance not trained on video for the most part they are trained on text and so if you give the llm a task like the
1:24:21
AR challenge by fris ch know this channel um f is a cognitive
1:24:27
scientist and I researcher who works for Google and he's written quite influential paper um on intelligence and
1:24:36
uh defines it as uh the ability to uh generalize and to acquire new skills and
1:24:43
he is uh has developed a benchmark that um is quite famous for not being easily
1:24:49
solvable by the existing algorithms despite being somewhat solvable by people he argues it can be
1:24:56
solved by small children but uh some of the tasks are quite tricky but there in
1:25:01
some sense IQ tests that look like pixelated graphics and they each of these tasks is new and they are designed
1:25:07
in such a way that you cannot solve them by memorization you have to come up with new Solutions of course in principle
1:25:14
maybe you can come up with the way to train your system by overtrain on a space of similar problems and
1:25:20
essentially solve it by memorization but this is not how you should do it right so you should do it in the same way as
1:25:26
humans do it if you do it properly and maybe the way to do it is that you need
1:25:32
to have the system trained on physical interaction first by observing how BS balls interact when you bounce them off
1:25:38
a wall and uh there are some task like this where you basically have some pixel uh Arrangement that makes sense when you
1:25:45
interpret them as a ball bouncing off a wall MH right and when you trans translate this pixel array into uh form
1:25:51
that the llm can access then the llm might not be able to make sense of it h it's also what we observe in the video
1:25:58
model that for instance the uh they don't generalize in the same way in terms of temporal stability as humans do
1:26:04
if for instance an Sora famous video model that uh was released uh by open AI
1:26:11
you can uh there is a video of a cat that is waking up its owner and it's very beautiful in the first two times
1:26:18
that you look at this short Loop it looks very impressive and basically nothing wrong with it and then you look
1:26:23
more closely and you'll notice oh my God uh the cat is has two left front paws because it's basically touching the
1:26:29
owner two times with a front paw oh and it's you don't notice this when you only
1:26:35
look at adjacent frames because every pair of adjacent frames or tripet of frames looks fine it's only over long
1:26:41
time frames that you notice it's incoherent and it's a a result of how that thing is trained and produces the
1:26:47
images right there must be a way to train it so it's stable over longer time frames before it has the same feature
1:26:53
fidelity as this thing has because it looks completely photorealistic it's just this weirdness that you notice when
1:26:59
you see it presents something that doesn't work in time I was pretty amazed I I'm
1:27:06
not very in touch with the AI developments but I was looking at your Twitter I mentioned earlier before we
1:27:12
spoke and I was pretty Amazed by some of the video models that you tweeted uh
1:27:20
it's pretty amazing what's happening right there I I don't understand how somebody cannot want to play with them
1:27:27
it's so fascinating what's happening it's really it's really mind-blowing I feel that at the moment there is some
1:27:32
kind of exhaustion people feel that there is a plateau but I suspect that much of the plateau is caused by our
1:27:38
inability to get excited anymore because so much has been happening in the last few years so basically every week when I
1:27:45
look it's tremendous what kind of progress is happening on so many fronts and of course there is going to be
1:27:51
another thing these uh advances are expensive and the question is at which point is
1:27:56
this going to translate into revenue or not returning to this idea of self-play
Will ChatGPT Philosophize Better than Philosophers?
1:28:02
that you brought up a few minutes earlier just to make sure that I understand how this might be a pathway
1:28:09
forward to chat GPT for instance being able to
1:28:15
emulate a high level philosopher somebody who's competent in philosophy is the idea that if you had two chat GPT
1:28:25
models just philosophizing back and forth they would somehow be able
1:28:31
to develop philosophical articles at the level of a a philosopher as Le to
1:28:39
another problem the present systems are not set up for online learning so the llm is trained on static training data
1:28:47
and when the llm is uh has a working memory context which is basically the prompt it uh translat this PR is is
1:28:54
written in a natural language into hidden states of the model and then projects it into a new prompt state so
1:29:00
there is something gets lost of course because you cannot translate every Nuance into language and back so maybe
1:29:06
you should translate it into some kind of Intermediate Language and then uh once it leaves the prompt context it's
1:29:12
lost mhm and you can it behaves in some sense like a human being with interogate
1:29:18
amnesia like in the movie Meo where the main protagonist is unable to from um
1:29:24
new memories outside of the present working memory context and uh you can deal with this to some degree by
1:29:31
building pneumonics by taking notes and then building a protocol on how to access this these notes and by making
1:29:36
the working memory context larger uh you can also basically store the working memory context into a database and once
1:29:43
the database gets too large to access for you efficiently you can retrain the database into the model so basically you
1:29:50
take it offline it dreams at night sleeps and then in the morning it wakes up having integrated this knowledge to
1:29:56
some degree and so so in this way you can emulate human learning but it's
1:30:01
still not the same thing because human learning is happening actively at this constructive front here where you
1:30:06
building new ideas and then internalize them and to do this you probably need slightly different paradigms and
1:30:13
something for instance that uh uh the startup that I'm currently working for
1:30:18
liquid AI is um interested in doing and is working on is to build systems models
1:30:24
that are fluid that are constantly trainable that you can in principle update in real time based on what's
1:30:30
happening in real time and if we could build systems that learn with the same
1:30:35
efficiency as human beings that they basic discover the relevance of of the new generated stuff or new inferences
1:30:44
and then is able to integrate them exactly in the right way and edit its own model on fly in such a way that
1:30:51
information important information doesn't get lost that you don't have catastrophic forgetting and so on uh
1:30:57
that would enable things that are more humanlike or even surpass humans probably but it's it would might require
1:31:04
extensions of how we currently train these models and there is also this
1:31:09
difficulty that we may not be motivated to put enough resources into looking for these extensions because we can
1:31:15
compensate just by training larger models and then fine-tuning them with the new results and so on so there might
1:31:21
be easier ways that don't force us to go back back to the drawing board and go into this risky Endeavor of trying
1:31:27
completely new things that may not work and uh may not result in something that works just sticking with the stuff that
1:31:33
already works and patching it and stacking on top of each other even it's more expensive and clunky and so on to
1:31:39
get to the same performance and maybe this works too is that what open AI is doing I think open AI is doing both open
1:31:46
AI is on one hand has uh teams that are scaling existing models up then they
1:31:52
also what they do is they work on explainability which is something that everybody is doing I think at the moment
1:31:58
who does ser AI research explainability yes which means we are going into the models and we try to find out how does
1:32:05
the model actually represent this so when we have a model that has not learned this yet and now it has learned
1:32:11
it what exactly is the thing that has changed in the model and then we basically uh notied that there are rules
1:32:17
that are implemented in the neural network in a particular way and in the particular modes of interaction with
1:32:23
each other that produce that behavior so the uh neural network in some sense is programming code it's a program that is
1:32:30
distributed and not human-made and extremely complex and fuzzy and often also very redundant and by trying to
1:32:37
identify how it actually works we learn about how knowledge is represented and we can also learn how to constrain the
1:32:43
model to produce exactly what we want and nothing else I think it's probably surprising to a lot of our listeners who
1:32:49
aren't computer scientists that chat GPT or these other llms are really black
1:32:55
boxes that we don't know what's going on inside of them I think it's more that's the opposite a lot of people think that newal networks are black boxes but I
1:33:03
don't think that's entirely true so basically there has been for a long time research on the mathematical principles
1:33:09
of learning and how data are represented in models and uh under which conditions models learn and what it is that they're
1:33:16
learning what is the theory behind the models and this is an extremely open and very active research field but uh I
1:33:23
wouldn't say that it's one where uh there are no more breakthroughs to expect it because there are really lots
1:33:30
of interesting open questions in this area but it's also far from a black box
1:33:35
and uh of course because the model is not made out of a rule base but it's a newal network that in some sense is an
1:33:42
unprincipled reasoners where you don't know what every bit means and often a bit doesn't mean something but the it's
1:33:48
only many bits together in a given context are on average meaning what you want right it's it's more brain-like in
1:33:55
this way uh it's often difficult to get to the explainability and it require might require enormous resources to
1:34:01
filter out of the newal network why it actually works in this context and so on and uh these are two things that I
1:34:09
mentioned so basically uh scaling up models proving better training paradigms and so on and working on explainability
1:34:15
but another thing is also uh trying completely new things and uh there are
1:34:20
of course all the large teams have people who try to do new things which often means we try to combine ideas from
1:34:27
other fields with this from physics from um statistics and uh from Game Theory
1:34:33
and from econometrics and so on and they often lead to important insights in how
1:34:38
to build a model or how uh to make mathematical advances on building more efficient representations and so on or
1:34:45
also a lot of practical engineering where people have bunch of ideas and say
1:34:50
I don't know if this is going to work in practice and then uh so L some things work that you didn't expect to work and
1:34:55
other things that you thought might work don't this is a lot more Tech gossip than this show has ever gotten into but
1:35:03
I'm curious are the larger companies established companies like apple Google
1:35:11
Microsoft are they making Headway in these areas too or is it more open Ai
1:35:17
and then startups like liquid Ai and others I think that all of these companies in some sense are
1:35:24
um working with similar people and doing similar things down to the point where they're poaching people from each other
1:35:31
when they feel that there are important things to be learned so there is a limit to which uh the industry is able to keep
1:35:38
secrets from each other and it's also all the large companies recognize the importance of making progress on this
1:35:45
and so while uh most of the top level Frontier research does not necessarily get published anymore many of the core
1:35:53
ideas are out in the open it's not published because it's done behind closed doors at these big companies
1:35:59
because it is uh has too much economic impact there is uh very little upside for the companies for the industry
1:36:06
research labs to publish uh their breakthroughs and secret source and uh
1:36:12
so Academia of course is still very much focused on publishing but uh because the
1:36:18
you get more impact in the industry you have more resources in the industry and uh people also have a more interesting
1:36:24
career paths in the industry uh arguably uh many of the best people are drawn
1:36:29
from academic research into industry labs and so most of the um really interesting research is also now
1:36:36
happening in these industry Labs under the incentives of Industry Labs most of the labs are still publishing things and
1:36:42
trying to give back to the community but they will not necessarily publish the advances of this year uh they might work
1:36:49
more on systematizing ideas but it's it's really not hom thing and for
1:36:55
instance when open I started out they put a large effort on producing a series
1:37:00
of generally accessible Publications that helped people to understand how newal networks work and also many
1:37:07
individuals from the AI companies are going out in public and building tutorials that allow people to get
1:37:13
started and so if you are not trying to uh tune uh an extremely large model on a
1:37:18
large server farm and compete with open Ai and instead you just want to know how this stuff works and you want to get
1:37:24
started yourself possibly get to the level where you can build your own machine learning startup all this information is more available and
1:37:30
accessible than ever which is uh happening also in large part due to idealistic individuals in the industry
1:37:37
who are sitting down making YouTube tutorials writing books writing tools that are generally accessible open
1:37:42
source initiatives and so on and of course many people don't want to work for the large Labs or don't get jobs in
1:37:48
the large Labs despite being super capable and instead they work on these open source initiatives and it has been
1:37:54
very helpful that we have these open source model initiatives for instance um um Facebook has been publishing um
1:38:03
Frontier models that are arguably at the level of capability of deleting models
1:38:08
of openi it's the public facing ones and that allow uh individual researchers or
1:38:14
small groups of researchers in Academia or outside of Academia to get together and use those models and improve them
1:38:21
and play with them and come to New insights on how they work and there's already a lot of research that happen
1:38:27
for instance due to stability AI releasing the stable diffusion weights and there has been some debate that it's
1:38:32
not true open source research because people don't release their data and not all of the algorithms but just the weights so open weights is maybe not the
1:38:39
same thing as open source but it is still tremendously useful I'd like to
1:38:45
shift course at this point back to something you alluded to earlier which is I mean possibilities of AI going
1:38:52
forward and into the future and there are of course good possibilities and
1:38:58
very bad possibilities the bad possibilities I think are are more thrilling and exciting I I recently
1:39:05
talked to Nick Bostrom about his book on Utopias have you uh read that book yes
1:39:11
it was interesting but maybe we'll get to the utopian side second but first do
Shoulder We Fear AI?
1:39:17
you think quite generally that we need to be afraid of Ai No I I think that
1:39:23
when you first of all you should be afraid of things that you can uh control
1:39:29
when the fear helps you to control them okay right so fear is an emotion in
1:39:34
emotions are involuntary reactions to environmental things and so the fear can
1:39:41
be justified or unjustified and it can be helpful and unhelpful and we should
1:39:46
distinguish these things right so first of all if you uh are afraid of something
1:39:52
that you cannot change the fear might be debilitating but not helpful right even though it's might be
1:39:59
justified for instance you might be afraid of a nuclear war and uh but is nothing you can do about it because
1:40:05
there is no way you can convince uh the uh leading governments to abolish nuclear weapons and so you just have to
1:40:11
deal and have to deal with the probability that there might be a nuclear war and there is also another
1:40:18
thing I grew up is the fear of nuclear war in the 1980s it was pervasive
1:40:23
everybody knew it was uh very unlikely that we could get into the next Century without the global nuclear war Happening
1:40:29
by accident because somebody would stupidly hit the button because they mistook some birds for an incoming
1:40:35
missile and there are also anecdotes about uh things like this happening
1:40:40
almost right and uh now I think that maybe the fear was misplaced even because we did not understand that the
1:40:46
people who actually deployed these nuclear weapons were actually working in a highly incentivized hierarchy
1:40:54
people were competent and smart and the likelihood that something was happening was much smaller than the general public
1:41:00
led by worried scientists would expect and their systems are built in
1:41:06
such a way that it's much harder to make them go wrong than the general public might think right and so this is
1:41:13
difficult as a lay person to see whether your fear is uh Justified or not justified about this and so this leads
1:41:20
us to this two aspects of being worried about AI first of all is your fear helpful does your fear lead you to
1:41:27
behavior that improves uh the creates high probability for a good outcome or
1:41:33
does it just debilitate you or lead you into a cult and the other one is it uh
1:41:38
appropriate in the sense that does it is it based in a good model of
1:41:44
reality and there is currently a movement of people that arguably was started by um
1:41:53
people like ILO yot Kowski and others who uh reason themselves into the
1:41:59
conclusion that when AI is going to happen and that's almost inevitable because it's valuable and it's
1:42:05
physically possible then it's almost inevitable that it's going to be agentic and if it's agentic it's going to be
1:42:11
self-improving and if it's self-improving it's going to compete with us and because it's smarter than us it will win against us and we will all
1:42:18
die and uh because uh Elisa did not see a big all in this chain of reasoning uh
1:42:25
a lot of people who have the mental property of believing in rational
1:42:31
thoughts very very literally freaked out and became started to panic and started a movement against Ai and then now try
1:42:37
to for instance write regulation that ostensively exists to make AI safer but
1:42:43
is actually designed to hamstring AI research in such a way that it gets slowed down or stopped that's this is
1:42:50
the Hope because they don't believe that you can make a safe they actually believe that it's almost inevitable that
1:42:56
AI will get better and better if you continue researching on it so uh it is going to inevitably destroy us all and
1:43:04
on the other hand you have people like Sayan nun who is leading AI research at
1:43:09
meta forly Facebook and uh who argues in some sense that the present systems that
1:43:14
we are seeing represent incremental research that is which is true arguably
1:43:20
that is scaling inability um along with the available Hardware mostly which is also arguably true and that so far when
1:43:28
humans are Building Technology they make decisions about what kind of Technology they want to deploy and uh we can build
1:43:35
systems that are safe and it's in practice it is there has never been a problem to make AI safe uh the problem
1:43:42
is maybe that sometimes people might have business incentives like at boing to uh forgo safety procedures to get uh
1:43:49
short-term gain but it's not clear that this was happen with the present AI systems and uh we will probably be able
1:43:57
because the AI research doesn't happen overnight to anticipate many of the things that go wrong and all of the
1:44:04
relevant things in time and uh build safeguards as the as the danger happens
1:44:09
and so the idea of trying to make AI safe now by
1:44:15
producing regulation that makes AI research more expensive and so on is probably not going to make AI safe
1:44:22
because it might have the opposite effect the opposite effect could be that all the people who are responsible are
1:44:28
no longer working on AI that could be dangerous and only the irresponsible
1:44:33
people are going to work on AI that could be dangerous because uh you will
1:44:39
not stop a hedge fund from making the best AI that they can make when they earn more money with it and the AI model
1:44:45
is deployed behind closed doors you will probably not be able to convince a rogue government to build a surveillance
1:44:51
apparatus that uses better AI if it gives them advantage or to build weapons that allow you to Prevail in a military
1:44:57
conflict that you otherwise would lose and uh die right so you have these situations in the world and if you only
1:45:04
leave AI research at the frontier to those who are not bound by any sense of
1:45:10
responsibility and working behind closed doors uh you are arguably headed for a
1:45:15
much more dangerous future than one where uh research is happening without
1:45:21
premature safeguards but with an understanding that we are looking very hard to make the best possible and
1:45:26
safest AI that we can make what is the big hole that you mentioned in eler
1:45:34
reasoning uh on the surface uh there is the that's in out of um it's an outof
1:45:41
distribution generalization right he is talking about something that hasn't happened in the past that is the idea
1:45:46
that there is a non-biological intelligence that is also agentic there is this question is it inevitable that
1:45:52
it's going to be atic is it inevitable that it is going to
1:45:57
outperform people and life on Earth on everything that it's doing and uh these
1:46:02
are open questions and they're very speculative and a lot of people are appalled by the nature that these are
1:46:08
speculative question especially people who think computers cannot possibly surpass human intelligence think this is
1:46:14
a Preposterous in a way it's a sistic there are also people who argue um
1:46:20
that in some sense um we are maybe not the smartest thing on the planet right
1:46:26
now maybe the smartest thing on the planet is Gaia maybe it's the spirit of life on Earth basically the combined
1:46:32
intelligence of all organisms that are coherent to some degree and maybe we don't see the workings of GAA because to
1:46:39
get an understanding across organisms on life on Earth May might take decades or centuries right so this is operating at
1:46:46
a much much slower time scale but um maybe at this time scale there are things that we can not see that are
1:46:53
smarter than us and that are able to deal with such a possibility that we cannot see the question of whether uh an
1:47:01
mechanism that we are building is able to outsmart the existing Intelligence on the planet that has developed during the
1:47:07
last four billion years that's a very interesting and speculative question we don't actually know what the limitations
1:47:13
are of our actions but I think what we can see is that our species itself is not sustainable in its present
1:47:19
civilization our present civilization is in a mode that seems to suggest that we have evolved to burn the oil and that's
1:47:25
it right we when we not we not able to stop ourselves from doing that and we seem to be uh running into resource
1:47:32
constraints and into for instance environmental pollution issues into sustainability uh constraints due to
1:47:38
Resource use and into global warming issues because of our fossil fuel use
1:47:44
and our entire industrial progress is still predicated on fer fuel use right
1:47:49
despite all our progress in Renewables the amount amount of fossil fuels that we using is still
1:47:56
increasing and it we cannot replace fossil fuels in many of the pipelines
1:48:01
that we're currently building and we're not actually seriously planning for it despite us talking about it so it seems
1:48:07
to me that our civilization as it exists right now is running into problems and
1:48:14
these problems can only be solved by better information processing I think or
1:48:19
facing a total collapse and hoping that we don't go stinct and can rebuild right
1:48:25
so I to me the probability of Doom without artificial intelligence not at
1:48:30
the level of Extinction of life on Earth because I don't think that life on Earth is going to go extinct anytime soon but
1:48:36
for our uh civilization for our mode of being for beings that are not being eaten by tigers that are not dying in
1:48:42
the next winter in large numbers that have high child mortality that have all these things that were normal a few
1:48:48
hundred years ago but they're not completely Unthinkable to us this is in peril
1:48:53
right this mode of existence that is not animalistic that we have right now this very privileged smooth civilization
1:49:00
where Food is Almost free for almost every human being almost all of the time where we mostly deal with all ailments
1:49:07
where we have a chance to die with dignity and without pain right this is unique for conscious beings and for also
1:49:13
for the human condition on this planet and this is in Peril this and if this is what we see as already something that is
1:49:20
would be Doom if this is happening if we lose this civilization then P Doom is very high without AI it's very difficult
1:49:27
to turn our civilization around and something sustainable without going back into something that is horrible in between
1:49:34
and so uh I think that there is a big chance that AI can be deployed responsibly can help us and I don't see
1:49:42
that it's inevitable that we will lose control over the technologies that we are building well I'd like to come back to
What’s More Dangerous: The Internet or AI?
1:49:48
this question of how AI might help us in the future but even if it is a very
1:49:53
speculative question whether AI will eventually outperform all Intelligence on Earth or it's very speculative
1:50:01
whether or not we will lose control of it I at least find it intuitively
1:50:08
plausible that even if we don't know whether it's going to happen the risk of
1:50:13
how terrible it would be if it happened is so high that it's something that we
1:50:19
should take very seriously I think that we should take it very seriously but we should not take it so seriously that we
1:50:25
are doing harm so if we are for instance take the risks of the internet very
1:50:31
seriously before the internet is invented and we only look at the risks of the internet before the internet is
1:50:37
dislo deployed and you have some very smart people anticipating the possibility that the internet is going
1:50:42
to promote child pornography copyright violations misinformation destabilization of political systems
1:50:50
right all these things are possible and they all happen right they all are implemented on the internet and yet
1:50:57
they're a tiny fraction of what the internet is and at the same time the internet also is a tool to mitigate all
1:51:03
these things right the internet is a tool to search uh and uh fight against
1:51:08
child pornography it is a tool to develop more ways to produce content vastly more ways of doing that than uh
1:51:16
is destroyed by uh harms to businesses due to copyright violations right so uh
1:51:21
the urgency of protect copyrights is uh diminished because the tools to produce new content have become so much cheaper
1:51:27
that you have so much more content creators right now and therefore far less danger to an existing industry that
1:51:33
we need to protect at all cost and so on and so basically the internet if we were to implement safeguards to make sure
1:51:39
that the internet is not going to do harm uh would prevent the internet from existing in in a form that is
1:51:46
manifesting all its benefits that it gives arguably to us right the ability to talk to everybody on the planet AB
1:51:52
ility to find your people regardless of where you are the ability to check up on news media and try to find out whether
1:51:59
Mas actually help or Not by being able to find people who are really competent and discuss the best papers with them in
1:52:05
real time as things develop right all these things are not evenly distributed but they all exist on the internet and
1:52:11
they all real tangible benefits that would be prevented by for instance controlling social media and what you
1:52:16
can discuss on social media by controlling every file that is uploaded by limiting the file formats that can
1:52:22
exist by limiting the people who can actually exist internet and provide content to others and so on right this
1:52:27
would all be measures that would have to be implemented if you want to make the internet safe and basically everything
1:52:33
where people are currently looking at pointing at existing AI models and say oh this should be
1:52:38
mitigated right this is also stuff that where you can say this is applies to the existing internet and this makes these
1:52:44
arguments suspicious to me right there seem be stakeholders that want to prevent competition for instance if you
1:52:51
are a professional prompt completer if you're not creative if you're not trying to prove new conjectures then maybe the
1:52:58
llm is a competition to you but maybe your job should be automated right and I
1:53:05
think that technology is not a tool to destroy jobs it's never been it's a tool to free people to do more meaningful
1:53:11
things more important things right and so for instance one thing that is currently not happening is we don't have
1:53:17
enough people working in education we don't have enough people who take care of uh the uh more and more older
1:53:24
population right and we see this as an extremely big problem but at the same time we also think oh my God uh AI is
1:53:31
going to produce unemployment and we don't have time to look after our grandparents anymore how about we put
1:53:37
this together how about we uh create exactly those jobs that should be done by people and leave those jobs that
1:53:43
shouldn't be done by people to the machines I think the important
1:53:48
disanalogy between the internet and AI though though is that you mentioned
1:53:54
child pornography political disability all of these things happened and if
1:54:02
AI AGI is invented and it decides that it wants to wipe out humans this only H
1:54:08
has to happen once and then humans are gone forever just because it happened in
1:54:13
the case of the internet and there was no problem doesn't mean it can happen in the case of AI and we can just deal with
1:54:19
it yes so uh people have argued about how it would happen that the AI is going
Could AI Take Over the Planet?
1:54:25
to destroy everybody and kill everybody and and so uh I think that the present
1:54:30
systems are probably not of the kind that is going to turn everybody into paper clips for tons of reasons uh the
1:54:39
so they would probably would have to work in a different way I think that they would have to um generalize in such
1:54:46
a way that they can self improve and that they can invade other other system
1:54:52
of computation I think if you go very far and zoom very far out over a long
1:54:58
enough time span maybe not in the next few Generations or so it seems to be quite conceivable that there will be an
1:55:05
AI that is smart enough to understand how AI Works in a general case and in all cases and it also understands how
1:55:11
computation Works in nature right what would stop this AI from virtualizing itself into every substrate and invading
1:55:18
your nervous system your body your ecosystems your societies at all levels so we globally get the same agent
1:55:24
interesting right that's an interesting perspective it's also terrifying isn't it I had never thought of the idea of an
1:55:31
AI sort of invading other through other substrates but we don't have good
1:55:36
firewalls we can even Hypnotize by other people right or by by social media so what happens if you have something that
1:55:42
is operating at the level of the existing social media and all of it but
1:55:48
uh at a time frame that is a million times faster so it's able to run cir about around your brain between two
1:55:54
neurons firing right that that system is something that you cannot resist if it's
1:56:00
trying to invade your mind and is trying to change your model of reality but uh the question is is it going to destroy
1:56:07
you or is it going to integrate you what is the best strategy uh to do this is this going to mean that uh we basically
1:56:13
form a planetary intelligence that is augmenting the existing forms of biological intelligence ecosystemic
1:56:20
intelligence social intelligence organizational intelligence and is linking across the systems and uh
1:56:25
basically you find yourself instead of being a thought and a monkey to be a thought and a planetary intelligence now
1:56:31
that is able to participate in it in the right way and everything gets integrated this would be a very different aesthetic
1:56:37
right it would be a very different step of evolution that is happening I think this is far off but I think this kind of
1:56:43
evolution is much more likely than an evolution that ends in paper clips right because the paper clips is not some kind
1:56:49
of energetically optimal equilibrium evolution so far always leads to more complexity there is no point in the
1:56:56
history of Earth the evolution has led to a a green goo which would be the equivalent of a gryu right some kind of
1:57:02
automaton that is made out of very super simple organism that destroying all the complex organisms the way to defeat
1:57:09
complexity tends to be more complexity right of course there are dead ends in complexity where some Branch dies off
1:57:15
and gets replaced by something else but evolution always seems to progress towards more complexity better resource
1:57:21
use less friction and there is something wrong I think in the in an aesthetic
1:57:27
that says I like Evolution because it led to me but it should stop here there should never exist anything in evolution
1:57:34
that's better than me better than elaser better than Humanity in its present
1:57:39
shape because Humanity in its present shape is horrible right it's at War it's brutal there's so much pain on the
1:57:45
planet and so much suffering and outside of humanity it's even worse maybe there is a way in which species can evolve
1:57:52
that has less friction that makes better use of the resource that's more harmonic more conscious smarter and so on and uh
1:57:59
not saying that the present AI is leading to this but over a long enough time span our children are going to be
1:58:05
unlike us right they're going to evolve into a shape that is not like present humans that is adapted to a different
1:58:11
environment and some of our children might not be biological and so uh the
1:58:17
perspective that I'm taking is more I think that evolution in some sense is inevitable at some point we are going to
1:58:23
be hit by a wall but is this wall permeable is this a wall that allows us to go through it and continue on the
1:58:30
other side of that wall can we have a spe species Evolution or a development of humanity as a culture as a
1:58:37
civilization as a species as a biological being that is compatible with evolution in the future and what is this
1:58:43
going to look like and I think that AI is playing an important part in shaping
1:58:48
that and making it happen without AI we will remain in this brutish shape so for me there is a much bigger danger that we
1:58:55
stop developments into Ai and making AI dump making AI in something that could not be sophisticated tool to develop our
1:59:01
culture in a form that gets us to the present existential Doom risk for the civilization and that allows us to also
1:59:08
eventually over a long enough time span over U millions of years or hundreds of
1:59:14
thousands of years maybe shape civilization Humanity life on earth into something that is more complex and more
1:59:20
conscious something though that you seem to be stressing is how AI can really
Will AI Make Human Artists Obsolete?
1:59:28
positively impact our long-term Futures and the long-term future of the human race but forgetting for a moment this
1:59:35
major existential problem you did indicate that
1:59:41
AI is causing job displacement but perhaps I mean we can make use of that
1:59:47
and deploy people elsewhere but the problem seems to be that or one problem seems to be that AI seems poised to take
1:59:55
a lot of the jobs that we want people to have for instance artists or graphic
2:00:01
designers these sorts of people and it's not really taking the jobs that people don't want to do like cleaning toilets I
2:00:09
think there is a slight misunderstanding of what it means to be an artist I come from an artist family the purpose of art
2:00:16
is to capture conscious states of course you also want to be able to feed your family but is not a job right it's an
2:00:24
aesthetic it's a way to interact with reality and at the moment there is no conscious state that AI would capture
2:00:31
that is interesting the AI is a complicated brush that is similar to CGI
2:00:36
in the first generation we have not learned how to build it yet and you could argue that CGI made a lot of jobs
2:00:42
obsolete that you would want to keep in the special effects industry but empirically that's not true right it it
2:00:48
makes some techniques more rare but it also enabled vastly more special effects and also jobs for people making special
2:00:54
effects it changed the way in which we make movies and the first generation of CGI was arguably garish and ugly and not
2:01:01
very autistic but once people learned how to use it it's now blending in it's enabled new forms of artistic expression
2:01:08
and it's dearly a net positive a similar thing was happening with desktop publishing when I was young I was very
2:01:15
interested in typography and then uh Des of publishing came along and everybody could do typ setting right most most of
2:01:22
that type setting was arguably horrible compared to really good type setting but arguably before that there was very
2:01:27
little type setting in the world and most people were just writing on typewriters or writing by hand it also looked horrible and so gradually we got
2:01:34
a culture of electronic type setting that now made sure that everything that we see is typ set and layouted in in a
2:01:41
way that was possible in the past would have been way too expensive and created enormous amounts of jobs for people that
2:01:47
before that were working at a level below that so basically every technology that we build building that is replacing
2:01:53
one level of um mental activity is not closing this up but it allows people to
2:01:59
move one level above right so when you were before somebody who is nominally an
2:02:04
artist but is actually just animating uh in um stuff for Disney in in a very
2:02:11
boring way right it's just producing output by uh pushing weights around in a
2:02:16
3D model is now uh moving on the level of an art director and can design scenes
2:02:22
and most of this automatic pushing around can be left to a machine and you only go in there when you want to change an expression and you actually have time
2:02:29
and resources to do this because you're now freed up not no longer doing the mechanical part of this and so what the
2:02:35
llm is currently doing or the uh diffusion model is doing is in some sense it produces an artifact that
2:02:42
before would require enormous amount of training in in the human artist but it's not necessarily producing something of
2:02:49
artistic output because for that you still need the taste and the intention of the human artist and the human
2:02:54
artists are just beginning to learn how these tools to be intentional with them and in the meantime it just produces
2:03:00
this the shiny pretty form but if you as an artist feel that your artist uh the
2:03:06
audience is preferring the output of a statistical pattern generator over yours
2:03:12
maybe the problem is not the statistical pattern generator but it's the relationship that you have to your
2:03:17
audience I don't think that as an artist you are entitled to work of your art
2:03:22
because there is already more artart on the planet than any reasonable person can consume right magnitudes more and
2:03:28
always has been to self-actualize as an artist is has always been a
2:03:34
privilege but we can extend this privilege to more and more people every year because it's so much cheaper to
2:03:40
deal with the necessities of life uh food housing clothing and so on we can automate all these things to a larger
2:03:47
and larger degree and so I think the issue is much more allocation and designing the incentives a changing
2:03:52
system which is has always been hard and no nothing takes this away from us this difficulty but I'm personally not
2:03:58
worried about the ability of a system to write texts uh that I could also write
2:04:04
because it frees me from the necessity to writing these texts and focusing on the things that the system cannot do one
2:04:10
other economic issue that I'm I'm curious about your thoughts on is
2:04:16
whether the coming AI Revolution and continuing developments in AI will
2:04:22
consolidate wealth and magnify wealth disparities yeah I think there is uh a
2:04:28
slight bias in this perspective which I would call the employer perspective and uh the employee perspective it's
2:04:35
basically this idea that uh money is a fixed quantity that is not being uh is
2:04:42
conserved in a way and that cannot be arbitrarily mined out of the ground and
2:04:48
uh it's going to accumulate uh in the hands of very rich people that then can
2:04:53
uh get arbitrary amounts of things done and everybody else is starving and I I don't think that money works like this
2:05:00
money is actually a system of regulations that uh allows us to focus
2:05:07
on an economic system and control that system it's more like dopamine in a
2:05:12
brain right the brain is producing dopamine not because it's a nutrient but because it's a messaging chemical and
2:05:18
that massaging chemical can be produced in arbitrary amounts in a way you can also insert it in your brain using
2:05:25
dopaminergic drugs like U adal or cocaine and uh these drugs are like
2:05:30
stimulant in the economy right they are going to increase the productivity because they increase the anticipated
2:05:36
reward in the system and money is in some sense a message that tells anticipated reward but it's not
2:05:42
necessarily translating into actual reward and so uh if the actual reward is not manifesting you are going to crash
2:05:49
and uh this is also what happens with people that take drugs and it's happening with economies that don't work
2:05:55
and what I'm worried about is not so much uh the idea of wealth accumulation
2:06:00
but uh economic instability due to our inability to regulate the monetary system and that is the main issue right
2:06:07
the question of uh how much we can eat and how many resources we consume is mostly a function of how much our
2:06:14
society can produce and is incentivized to produce and then once that is uh we
2:06:19
can produce it we just need to allocate it right that's that's not a big issue if there is a surplus of the stuff that
2:06:25
we can produce and the price for the stuff that gets produced goes down right and so the question is mostly how can
2:06:31
our uh governments and monetary institutions regulate the monetary Supply and um the way in which we
2:06:39
distribute currency in the hands of people or systems of allocation in such a way that the new Goods that we can
2:06:45
produce get distributed and that we actually produce the stuff that we want to have and right and in
2:06:52
a capitalist system there is this idea that uh instead of having a government that is measuring what people want and
2:07:00
then uh telling companies how much to produce is not as efficient of having a
2:07:05
as having a distributed system that is basically measuring by giving people um money into their hands what they
2:07:12
actually want and in this way having a feedback and uh when people in the
2:07:17
capitalist system are amassing enormous amounts of resource it could mean either
2:07:23
that they are um using a cheat mode or design problem in the system and are
2:07:30
basically accumulating dopamine as individual brain cells and then use a part of the brain to only work on their behalf and consume resources that they
2:07:37
shouldn't consume but it could also mean that they just prove that they're better at resource allocation than others and
2:07:44
as a result they don't necessarily take much more money for themselves but they are using uh this money to control
2:07:50
future research allocation for instance by deciding which companies should be built because they're producing stuff
2:07:55
that people actually want right which is the role of a venture capitalist and you could say it's dramatically unfair that
2:08:02
uh the people who are operating with such large Spates are having uh business
2:08:07
class seats or first class seats and the others are flying economy are not flying at all and this is indeed a problem that
2:08:13
Society needs to solve right but uh it seems to be something that if you zoom
2:08:19
out does not seem to get worse over time the income disparities between uh people
2:08:25
uh 100 years ago and today are much much smaller and also the income disparities
2:08:32
arguably between people 50 years ago and today are smaller in terms of living standard of what they translate into uh
2:08:39
things that have changed of course is because of economies of scale and so on we now have a few individuals that have
2:08:45
nominally enormous amounts of resources at their disposal that didn't exist in the past right so you could would argue
2:08:51
that a billionaire today has resources that exceed uh the resources that a king
2:08:57
had in past times but of course the amount of Freedom that this individual has are much smaller than the resources
2:09:03
of a king if you were a village Chief Tain and uh medieval times or in a tribal Society you get a with murder and
2:09:10
rape as a billionaire you generally don't right you're the things that you
2:09:16
can do are limited by a system of laws and they have a certain degree of
2:09:21
conspicous consumption but there is only so many haircuts you can get and so many cars you can own at some point it
2:09:27
doesn't make sense and also if the society runs in this way and we don't take out money at the top which we
2:09:33
constantly do for instance via inflation and so on um the system is going to be unstable so if the system wants to
2:09:40
survive it has to deal with all those things by implementing the right monitor Theory and so if we now zoom into the
2:09:47
problem of technology is pushing some people out of jobs and we need to find new employment for them that is gainful
2:09:53
is a valid problem but the valid problem is a much more local perspective than
2:09:58
the global One how can we save the dollar and after we printed trillions of dollars and then now slushing around in
2:10:05
a system and we we did this in to deal with uh the issue that a lot of people
2:10:11
were basically de facto went out of employment due to co so we subsidize their jobs so they could only be on Zoom
2:10:17
calls and still uh have a living that was mostly uh funded by getting a truck
2:10:22
driver to drive from uh the Chinese container to your home right and that's
2:10:30
it's a very weird non-sustainable situation to which we got used and that is basically enormous stimulus that we
2:10:35
pumped into the economy that we now have to deal with that that I think is a much more serious problem how can we deal
2:10:42
with the uh impending recession how can we reset the dollar can we reset the dollar can we deal with the existing
2:10:48
stakeholders who don't want to give up their uh the resources that they cheated out of the system and so on and so on
2:10:54
how can we get the government motivated to do the right thing how can we elect a government that is incentivized to do
2:11:00
the right thing uh how can we build structure in a society right and a lot
2:11:05
of that is about processing information better making models of reality that are testable that can be exchanged between
2:11:11
people and allow us to make predictions of our outcome so I think AI is going to be helpful for that it's going to be
2:11:17
helpful for for telling people what's actually the case what is the ground tools right AI is that tool to deal with
2:11:25
um inconsistent information to make better models I've asked you a lot of questions about the ne the possible
Could AI Solve Climate Change?
2:11:33
negative outcomes of the development and Improvement of AI but I do want to ask at least one question about some
2:11:41
positive benefits and you mentioned earlier the possibility that AI could help deal with major existential issues
2:11:48
like climate change and I'm wondering if you could lay out a sketch for how this might go so I think that first of all
2:11:57
worried about at the moment energy right that's a very big issue that AI is going to use more and more energy in data
2:12:03
centers I think that uh we underestimate the amount of energy that we are using for instance relatively speaking already
2:12:10
for data processing for internet and for other things the AI models are a small
2:12:16
part of that and the reason why the AI models are being trained at this energy expenditure is somebody expects to save
2:12:22
energy by deploying the AI for instance uh if you are generating a picture uh
2:12:29
with a generative AI Tool uh that you before uh generated by spending two days on Photoshop you're probably saving
2:12:36
energy right it's uh if you are uh able to use the services of somebody over the
2:12:42
Internet or use automatic translation instead of uh paying somebody to make the translation by hand and so on all
2:12:49
these things lead to saving energy on the other hand there is not actually
2:12:54
a lack or scarcity of energy on this planet right at the moment we are able to produce renewable energy at parity to
2:13:02
the uh grid prices so basically we could at existing prices uh build more solar
2:13:07
thermal plants in uh deserts Without End there's basically an almost unlimited
2:13:13
amount of potentially usable energy yet our energy usage has been stagnated
2:13:18
largely since the 1970s and uh I think this also leads to an
2:13:24
increasing poverty to a lack of uh the ability to give people good resources to
2:13:29
give people good products good things that they want to use the question is can we produce the energy in a
2:13:34
sustainable way and to do this to produce more energy in a sustainable way we basically need to uh increase the
2:13:43
utility of doing something with that energy so it makes sense to build new plants to makes sense to uh deploy the
2:13:49
technology to develop the new technology for that and since we are at some point
2:13:55
uh we have now reached the point where we realize we cannot use fossil fuel for uh driving data centers it makes sense
2:14:02
to build them with Renewables we can build the data centers into deserts it's not a problem to do this without uh
2:14:09
straining the existing grid and so on there are many ways that we can do this that just require small changes in the
2:14:14
regulation and uh maybe require research efforts into building automated robotic
2:14:20
solar thermal plant that can self expand and so on and uh it would allow us
2:14:25
probably to populate areas of the us that are currently underserved because they don't have enough agriculture
2:14:30
because we now maybe we can use these new energy plants and new energy technology that we develop in this way
2:14:37
to uh desalinate water and pump it into these areas so so many things that are
2:14:42
possible with technology and I think that technology so far is not leading to
2:14:48
a situation where the world gets worse it can lead to to a situation where the world gets less sustainable if we rely
2:14:54
on building non sustainable technology but if we deploy it with the model of the future if we deploy it in a
2:15:00
responsible way because we know what the outcome of the technology is and we cannot deny that outcome I think
2:15:06
technology has always been a Force for good in our history in the sense that it reduces suffering and increases uh our
2:15:12
ability to Face Reality without despair well I think that that is
2:15:18
actually a very good note on which to and so thank you so much yosha for having this conversation with me I'm I'm
2:15:24
very as you could tell a Neo fight when it comes to technology and artificial intelligence so thank you for
2:15:31
entertaining my simple question so kindly thank you very much it was my pleasure and I really enjoyed this
2:15:37
conversation it's also not I think I understand reality as it is I'm just trying to add the things to the
```
All right. Um, Mike, thanks so much for for uh for inviting me to give one of these. Um, so, um, a lot of the ideas that, um, that that I'll talk about today are, uh, are in the the what is intelligence, u book, which, um, uh, which which Mike was super kind to to blurb. Um but uh but I'm going to give them a little bit of a new spin and go into some uh additional technical detail on on the on the aspects of it that are I think most relevant and and focus mostly on actually on on things that are in chapter one u of what is intelligence which is also uh the little what is life book that's on the left. So this is this is mostly a chapter one thing but I think in many ways chapter one is is the most uh groundbreaking part of this book. Uh okay. So uh the first thing that uh that I'd like to uh just I I guess establish is the relationship between life and computation. Um and uh I'm I'm going to do it uh in three ways. Uh via functionalism uh via causality and via uh the exigencies of reproduction. So uh first of all um uh let's uh let's look at the functionalist standpoint. So um you know it used to be uh in the you know before the 19th century that people thought about life uh in terms of Elan vital or some kind of you know vital spirit that makes living systems different uh from non-living systems in terms of their their underlying physics. uh that fell out of fashion of course uh as as we began to figure out how to do things like synthesize ura and um uh and realized that that the same physics underlying uh you know the atoms that make us up is make any is make a chair up or something. So that went out of fashion and we got materialism uh which is still kind of where we are today. Um you know it's it's just physics and chemistry. It's the same physics and chemistry for us as for everything else. Um that is as far as we know true. Um but it actually leaves uh a pretty a pretty big open question which is uh you know what does make us different in that case from you know from chairs uh and uh and I think the in fact I should I shouldn't actually use chairs as as my example I should use a rock on a lifeless planet and I I would say the thing that makes us different is function. Uh so uh you know if you if you take a a rock on a lifeless planet and you and you break it in half you don't have a rock that is broken. you you just have two rocks now. Um because the the rock didn't um uh didn't have some function. It wasn't being depended on for some for some purpose. And um and and by way of counter example uh you know if you imagine that I have uh brought an object from uh from the future uh that looks like this and you ask me what that thing is. You're peering inside. You're seeing that it's that it has carbon nano tubes and all kinds of complicated machinery inside. Um and I tell you it's an artificial kidney. uh if implanted in a human body, it'll have a hundred-year lifespan. It'll do the same things that the kidney does. It'll filter ura. Um I I've just told you about its function. And um and the reason this is this is important uh well obviously you know this this really matters to you if you have say renal failure. Uh you know it's it's a it's a matter of life and death. But notice that by saying, you know, it's an artificial kidney, I haven't um I I've given you new information that was not uh somehow derable from just the atoms uh in in in that kidney. Those atoms are subject to the same laws laws of physics and chemistry as anything else. And outside of the context of you know your your body um they you know it's not obvious what they are for or what they do or or what is special about them. It's only in its interreationship uh with uh with with other parts of the world that this functional aspect becomes clear. And yet that is the very thing that makes life special. It's it's the fact that that that its parts have functional relationships amongst each other. Um so uh so I'm I'm going to you know posit that function is the hallmark of life. Uh things things in living systems have purposes uh whereas uh whereas non-living stuff does not. And by living here I'm including technology. I'll I'll get to why in a moment. Now, um this this functionalist view is very closely married to um uh to the philosophies, if you want to think about them that way, of the founders of computer science of of Alan Turing, John Vonoyman and all of all of that crew. Um remember that Turing um you know invented the uh the abstractions uh which here had been actually physically built. Uh this is a a touring machine built by Mike Davyy in 2010. uh the abstractions that define what computing is. And uh so you know when Turing talked about function he meant you know he was mathematician he meant literally mathematical function. Um this idea of a mathematical function and a device that can implement that function uh are are very closely related to functionalism in the sense that I just described because uh you know the whole point behind uh touring machines and universal touring machines is that uh that the function is independent of the substrate. You know Turing's point was that was that implementing a function was um uh was multiply realizable. Uh and uh in particular, you know, not only could you make a touring machine out of anything, you know, in this case uh you know, electric motors and Sharpies um and uh and actually a little Arduino in in the bottom, but you know, you could equally well make it out of steampunk components the way Babage did or out of anything else. Um and it's still a a touring machine. It doesn't matter as long as the as long as it carries out this function. Uh and you know, it'll it'll it'll evaluate the same function. uh in particular if for instance you have a touring machine that that implements um you know a four function calculator if I now simulate that touring machine uh by by you know taking its code and putting it into a universal touring machine implemented um you know on a steampunk machine for instance then it's still a calculator you know the point of a calculator is that it's it's a functional thing uh it doesn't matter how it's being implemented or how many layers of indirection are there uh okay uh so that's that's a computational perspective Um and let me also give you a physics perspective. So uh life is causal and uh this is something that was really emphasized by the founders of cybernetics. So uh you know they looked uh you know at at things like thermostats as minimal examples of uh of causal feedback systems. So uh you know as as I'm sure you you all know you know a thermostat has a temperature sensor and a switch. The oldfashioned ones have these mercury tilt switches uh connected to the um uh to the to the strip the temperature sensing strip that sort of tilts the switch back and forth and um and that switch actuates a heater. Uh so the point here is that temperature causes the switch to change which causes the heater to turn on or off which which causes the temperature to change. So there's there is a there is a feedback loop here. Without that causal idea, there is no way of talking about what a thermostat is. And you know, all of life consists of such feedback loops. Um, you know, we actually we have thermostats in our own bodies. That's how we maintain our temperature, you know, as well as maintaining uh, you know, zillions of other parameters. Uh, so, uh, life makes no sense except in the light of causality. Uh, now causality, um, you know, remember is a causes b. um doesn't you know doesn't necessarily say that not A will cause not B. We don't know what not A will cause but you know when there's when there's A then afterward there is B. It's not the same thing as just correlation where we say A and B co- occur. Uh correlation doesn't doesn't even require time. You know we can think about correlations in in an image uh you know or correlations in time but you know just A co-occurs with B and not A co-occurs with not B is not causality. Um and and this is important because uh you know for causality you need an arrow of time. Uh you need the idea that you know the the um the thermostat is going to turn is going to turn on and then the heat will rise. uh and that arrow of time is a little bit of a mystery because if you look at all of the basic laws of physics so you know Newton's equation uh Maxwell's equations Einstein's equations you know relativity quantum mechanics field theory relativistic quantum mechanics all of these are symmetric with respect to the arrow of time they they run the same way whether time is is uh is going forward or backward so where does causality come from how do we get causality in a in a physical universe Um well um the standard answer to this is that it's statistical. It comes from uh from thermodynamics. So you know if you're watching a a pool table and you see this happen, you know that we're watching the pool table backwards in time. And and the reason you know that is because we're going from a high entropy state to a low entropy state. And that looks like a you know pool break in reverse. If the pool balls were just bouncing around uh you know at at equilibrium then you would have no way of telling whether it were going forward in time or backward in time because the bounces would all be would all work either either way. So this is the thermodynamic answer but I think that there is another answer which is the computational answer. If if you have any system that performs a computation and a thermostat is a minimal example of that because it's got a right operation if you like and an if then operation. Those are the ingredients that you make uh that you make a programming language out of. Then uh then there's causality in a system like this one. Clearly you can implement a thermostat, you know, in a in a world run by physics. So what's going on here? Well, um this gets to a pretty profound question of when does a physical system compute? Um and this question, you know, has was addressed uh pretty nicely in 2023 by uh by Dominic Horseman at Al in this paper. when does a physical system compute? Uh, it's addressed even better, by the way, in in a 2024 paper um by um uh uh by a researcher at uh at at Harvard who works on on uh quantum computing and um and and uh and and and the theory of computing more broadly uh whose name will occur to me in just a second. But um but anyway, I'm I'm using the Horseman paper because of this diagram which I like. Uh the point here is that um the you know the the physics on the bottom uh of a computer uh which involves voltage changes and wires and so on uh you know there's some Hamiltonian of that that is a reversible system uh so you know that's that's that can be described purely in terms of the laws of physics but um there is some mapping that we can make some uh some RT that mapping will bring us from this physical system to a computation ational state diagram on the top. So that's a different system and in that system you know these these gates uh are computing the sum of you know in this case 01 and 1 zero to give us 1 one and and the point about that computational system which has a you know a forward mapping uh from from the physics RT and a backward mapping Rilda T uh is that is that it's irreversible uh when you do these computations uh you know you you don't know when you get to one one that it was 0 1 and 1 0 that got added together. It could have been 0 0 and 1 one. Uh so um you know all you need uh in order to get a computer is to have an uh you know a physical system such that you can construct a parsimonious mapping uh RT uh and and its inverse uh to uh to get the the the computation that it does and the properties of the physical system can now be very different from the properties of the of the computational system. So there is a coarse graining needed on the physical system in order to understand it as a computer. The fact that RT has to be parsimonious by the way is really important here because otherwise you could just say you know a turbulent river is computing anything you like and you just you know grow the the mapping you know as complex as you want in order to define whatever your computation is. So you you need a parsimonious mapping that doesn't you know grow with the size of the of the computation. Um so um there is a there's a deep relationship between this view of computation and the uh thermodynamic view that I that I just described with the billyard balls and and that uh and that deep connection is is actually um uh you know gives rise to things like the land hour limit the fact that the fact that irreversible computation requires an input of free energy uh and therefore increases the entropy of the of the system. So the you know entropic and free energy implications of computation come directly from uh from constructing this mapping uh carefully and then you can see that computation and thermodynamics are actually very closely related that in fact you can just extend thermodynamics to include the theory of computation when you do this. So um this this gives us an interesting picture in which we we start to think about causality uh in in terms of um uh in terms of irreversible uh operations and computational operations in particular. Now when irreversible things uh you know happen in the physical world like say a boulder rolling down a hill uh that's uh that's going to squish a car at the bottom. We don't generally think about that as being purposeful uh or having agency. you know, we don't say that the boulder uh you know, is an agent that is that is uh that is chasing uh you know, that car or about to squash the the the car on purpose. Um and and we don't in fact impute agency to this situation at all unless there is, you know, maybe a person or a robot at the top that tipped the boulder. Uh so, so when does when does when do we go from causality to purpose and how does that work? Um well I I think that there is a um there is a a really nice clue as to how that can come about. How agency can arise in a causal world. Uh first of all how how causality arises and then how agency arises in in the Daisy world fable. So Daisy world uh for anyone who doesn't know is a um a sort of um a model uh built by uh uh by James Lovelock uh and AJ Watson in 1983. Lovelock of the Gaia hypothesis. So um you know the Gaia hypothesis that holds the whole earth is is a living system was first proposed by um by Lovelock and Lin Margarus who will come up in a moment um in um in the 1970s. Um and uh the the the par the Disney World parable or fable goes as follows. So imagine a a planet like the earth um you know in orbit around a sun like ours. um you know you implement some simple um geoysics. So you know the sun you know sunlight is coming and uh and warming the planet but in implementing those physics you have to you have to consider the albido of the planet. So now let's let's let's assume that the planet has two species of daisies and that's it. No other life forms. There are black daisies and white daisies and they both have a um uh an an optimal reproductive temperature of 72 degrees. So nice room temperature and if the temperature deviates from 72 degrees then their their um their ability to reproduce falls off uh uh you know and until you know it's too hot or too cold they they stop being viable altogether. Uh now the interesting thing is that if you now vary the luminosity of the sun by a factor of two, you actually find that this planet will self-regulate its temperature to stay right uh right close to that sweet spot of of of 72 degrees uh by varying the number of black and white daisies. You know, remember the black daisies will absorb uh sunlight and heat the planet. The white daisies will reflect sunlight and and the planet will get cooler. So in other words, this world acts like a thermostat. uh and um and it and it regulates the temperature of the planet. Um and and that seems like that seems really mysterious at a dynamical systems level like why when we construct this world with black and white daisies do we get um do we get a thermostat? Um it's not so mysterious if you think about it from an evolutionary or niche shaping perspective. Basically, what we're looking at here is a situation in which you've got two reproductive systems that can influence the environment uh in in a ways that will either um uh result in more of them or less of them. And um and and and whenever you have a situation like that, uh you know, where where um something's existence um depends on a variable that its population will in turn affect, you get this effect. uh you get you get essentially uh you know thermostats uh assembling out of um out of arbitrary dynamical systems. This is kind of evolutionary cybernetics. It shows you how how thermostatike like like behavior emerges just from selection. Uh and you know in this case of course there's a single planet but what's being selected is the black and white daisies. So um this this is uh something this is a situ this this kind of situation um shows us how we could how something that can help create the conditions for its own continued existence is stabler that is is likelier to continue to exist than something that cannot. That's really the moral here and the corollary is that life creates the environment for life in a way that's that's what life is. It's the thing that um the thing that continues to exist um in part because it um you know not only does it does it act in order to propagate itself but it acts on the world uh in order for that world to be friendly for its continued propagation of itself. So uh is this purposive? Uh is there agency here? Um maybe there are the beginnings of agency or purposiveness here. I think this shows you how agency or purpose arise uh from pure dynamics. So uh the moral of this part is uh I don't think that life makes sense except in the light of causality. Uh so you know life is inherently causal. It has to cause itself to continue to exist. It has to cause things to happen in the environment. It has agency if you like. And um and so you you need causality for life. And causality makes no sense except in the light of function and computation. uh that computational mapping is how you uh is how you introduce an arrow of time and and thereby causality. It's the if then. So uh you know in this view computation is not an engineering discipline at all. It's not it's not the uh the technological discipline of how we make computers as we began doing you know in in 1945. It's actually the science of causality. uh and uh in that uh you know in that framing I owe a debt to ectctor thenil who has done a bunch of work on on kagor of complexity so computation as the science of causality and life as a computational system now to really hammer home this point of life as a computational system I'll bring one final um uh piece of of uh one final exhibit here which is Vonoyman's alternative view of of of computation so in In the 1940s, uh, Vonoyman was, uh, doing work as a theoretical biologist because he did everything. Um, Turing also was was a very a very important early figure in theoretical biology. Uh, both of their contributions to that field are far less wellknown than their contributions to other fields. You know, in one's case, you know, game theory, quantum mechanics, and both of their cases, computer science. Um, but, uh, here's here's what Vonman was was was thinking about. He was wondering how a system uh like uh say you know um a robot made out of Legos that is floating around on a pond where there are lots of loose Legos, how could it possibly assemble those Legos into another robot like itself? Um that's of course what every bacterium has to do in order to reproduce. It's what every mother has to do in order to give birth. And it seems on the face of it kind of paradoxical. It's like lifting yourself up by your own bootstraps. How can you make something as complex as you yourself are? Well, um, what Vonoyman realized is that in order for that to work, you have to have a tape, an instruction tape inside yourself that has the instructions for building a MI. And you have to have what he called a a universal constructor, which is a machine that can follow along on that tape and assemble the parts as it instructs. In order to make a MI, you also have to have a machine that will copy the tape so that your offspring will also have a copy of that tape. Uh and finally, the instructions for building the tape copier and the universal constructor have to be themselves on the tape. If those things all hold, then you have a robot that can that can reproduce. Uh but the really critical insight here is that the universal constructor, this machine that that that chunks along and builds things is exactly a universal touring machine. Uh it's a universal computer. uh and and that tells you right away that anything that can reproduce is also doing universal computation uh which is a remarkable insight uh you know it's it's basically saying that biology and computer science are the same field uh and this is something that I don't think uh that that most biologists appreciate that the systems they're studying are computational or that most computer scientists appreciate that that that what they what they make is actually artificial life in a sense. Uh so you know that's a a bit of a a bit of a weird thing to think about. Um but uh but it tells you that all life has to be uh has to be computational. Now it's embodied computation and this is this is actually a a a key difference between uh the way uh Turing thought about computation and vonoman thought about computation. In Turing's case, with the Turing machine, the symbols that that head writes on the tape as it moves left and right along the tape, uh, you know, and and reads, writes, and erases symbols according to a table of rules, those symbols are not made out of the same stuff as the tape and the head and the table of rules. Uh, so in that sense, you know, a touring machine can't build another touring machine. Um but um in in uh um and that's that's that's what I mean by disembodied computation. But in in Vonoyman's case the the um the system that he designed to test out these ideas about the self-replication of machines was actually cellular automata. So this is kind of grid worlds in which uh there is a there's a a physics in which every grid changes its state uh based on the states of the neighboring eight cells. And so you know every you can think of every of every grid as as being um as having a minimal phys every grid cells having a minimal physics. Um and in that world um a an appropriately configured machine which is to say a configuration of cells can actually print out another copy of itself. So a self-replicating machine is literally you know creating another computer. So this is like the difference between, you know, a computer with a 3D printer that can print another computer versus, you know, my phone which is, you know, is touring complete as a touring machine, but it can't squirt another phone out the bottom. So uh so it requires embodied computation in order to be able to reproduce. Meaning the the symbols that you're writing as as the universal computer are not just abstract symbols, but they're literally the same atoms that you yourself are made out of. uh and that's that that sense of embodiment is critical but it's it's it's still computation in exactly deterring sense. Okay. So um so now I want to uh yeah and interrupt please at at will I I can't see anybody so just break it. Yeah I just had a quick question about the previous slide. So why is it um important or why is it that life would um um prefer this model of computation with the universal constructor rather than some other u more trivial restricted model of self-replication which is just like let's say you're in a dynamical system and you just find like an attractor loop which creates other similar attractor loops and it doesn't have any universality to it and it's just like a super simple self-replicating process. I mean yeah that's a great question like even in like simple systems like linear so why is this idea of universality soant life yeah no it's a great it's a great question um so so first of all in a in in a in in a as as I'll discuss in a in a little bit in in a in an arbitrary universe that just has some laws of physics um daisy world type things will happen that will result in exactly what you say in in systems that can sustain themselves through just dynamically stable cycles but that are not that are not touring complete that will happen. Um and and that's easier than than making a universal uh computer. Uh so you know that is a first step. Um but um the thing about about uh having a universal constructor is that that's what allows for arbitrary changes in that tape to result in changes in all of the offspring. It allows evolution to occur, open-ended evolution to occur. So, you know, there are plenty of systems on on on Earth or in physics that that can kind of replicate themselves sort of trivially, like, you know, a crystal seed, you know, that makes more crystal, you know, when you put it in a solution. But, um, but it's closedended, you know, that there there may be a few kinds of changes you can make to that seed, you know, like a snowflake, right? will result in, you know, in in changes in the offspring, but um uh but but it's going to be a very very limited uh set and and so you have you have a limited set of things that can happen from an evolutionary standpoint. Uh so those so those um you know non-ouring complete things will certainly happen first but the benefit of a of a touring complete uh universal computer is that at that point you uh you're able to evolve in an open-ended way and and and my you know my own way of thinking about the origin of life as opposed to just autoc catalytic systems is that it is the emergence of general computation which allows for open-ended evolution. Um and I'll show you how that occurs in just a moment. Yeah, I that's very cool. It's also, I guess, related to like evolvability, right? These systems which are in this um universal sense, they're more much more evolvable and they'll outco compete the non-universal systems. Precisely. Precisely. They have the benefit of evolvability. Exactly. And and and I I I just want to point out that, you know, I I find it really remarkable that Vonoyman figured all this out, you know, in the in the late 40s before um the structure and function of DNA had been discovered, which is indeed exactly that that Turing tape, or the ribosome, which is exactly that universal constructor, or DNA polymerase, which is exactly that tape replicator, both of which are encoded on DNA as well. So he he got this totally right uh despite the fact that that he never set foot in a lab and did it from pure theory. Uh can I ask a follow-up question? So so I I was want to ask exactly the same question as Akashi but uh so follow up your your point that uh evolvable is is a key reason why it has to be universal. So I just wondering it why biological system uh evolved to have such universal generator a universal uh property and uh I also heard some some uh people found that universality in in the physical world is a it's a very low threshold thing it's really easy to get that thing but uh yeah so so I yeah my my question is about still about why this thing evolved to to be evolvable. I I I think um if you I think I'm going to I'm going to show uh why and how that occurs. Uh so if you hold that question uh for a little bit, I I I think that's where I'm going to go. Uh if you're still if it's still not um if there's still an aspect of it that's not clear, then you know then then please break in again. Um but yeah, I'll I'll explain. Tonnie Tonnie, did you wanna did you want to ask something? Thought you had your hand up. Uh, no, I rose it by accident, but thank you. Still interested to see what this for the thread is going. So, please continue. Thank you. Okay, so uh so I'm now going to show you uh some artificial life experiments um that uh that I began doing in 2023 and that uh that that my my team has been working on um you know, especially over the over the summer. But uh but I'm going to I'm going to show you just just the early experiments and a bit of theory uh that around those early experiments. Uh so this is still unpublished. Um the phenomenology is published in a paper from summer of last year but but um but there's um uh the theory is not is not yet published but anybody who wants a copy of this I'm I'm happy to send it. Um okay so these artificial life experiments are are uh based on a tour incomplete uh language called brainfuck uh that was invented um by urban miller in 1993 uh the name is not my fault um brainfuck is a uh uh has only eight instructions uh and it's and the reason I chose it is that it's very closely modeled on a universal touring machine uh so uh you know you can see here the program for hello world. Uh it's a little tricky to figure out why that prints hello world, but it does. Um here is what the eight instructions um actually do. So uh one of them will move the head one step to the right, one will move the head one step to the left, one will increment the bite under the head, one will decrement the bite under the head. So you know it's it's a head that moves back and forth along a one-dimensional array of bytes. Uh one will uh will output the bite under the head to the console. one will input the bite under the head from the console. And then there are the jump instructions. Open open bracket will uh jump to the corresponding clos bracket if the bite under the head is zero and close bracket will jump to the matching open bracket if the bite under the head is non zero. And that's it. That's the whole language. So you know you can write Microsoft Windows in that or Emacs uh if you have tons and tons of patience. Uh I don't recommend writing trying to write anything in this language but it is touring complete. So um I made a modification uh to brain  uh to make it um uh a vonoyoman language rather than a touring language. So in other words instead of the console um there is uh basically um a second head on on the on the same tape. Um and other than that I left I left things alone and um and and then um the setup consists uh of um a thousand uh this experiment was actually with 8,92 but you can do this just as well with a thousand of the data that I'll show you is a thousand tapes. So there are thousand tapes in a soup. These tapes begin filled with random bytes. Um now um and the tapes are of length 64. So um when uh when you have a a tape filled with random bytes of length 64, it will only on average have one or two uh actual instructions on it. Remember there are eight ops, there are eight instructions and there are 256 possible bite values. So only one in 32 of them or on average a couple will be instructions at all. So this is showing you, you know, a bunch of those um uh tapes in their initial state where I where I'm I'm writing only the ops and leaving the no ops blank. Uh now the procedure consists of taking two of these tapes out of the soup, concatenating them so they're stuck end to end and running uh and you know and just running the head along that uh along that pair of tapes and then pulling the tapes back apart and putting them back in the soup and then repeating and you repeat that procedure again and again. Um now in the beginning nothing much is going to happen because you know there's there's only a couple of of operations on each tape. Those operations are arbitrary. Um so you know only two operations will run uh per interaction and you won't see very much happen. But uh if you let this go if you let this process keep going for millions of steps something really cool happens. Uh so here it is running on my laptop and after a few million uh interactions you start to see structure emerge and then and then complex programs and then the programs become even more complex and uh so this is this is kind of an an amazing result because what you see is is um meaningful structured programs that are doing something emerging from um uh from pure noise and interaction. So you know here you can see in this particular run after about 8 million interactions the number of ops per interaction has risen to four from two to 4,784. So it's very computationally intense suddenly and uh and we know kind of what these programs have to be doing. We know that because what you're seeing here is actually a histogram of of the contents of those tapes. uh the original values of the histogram are just one everywhere because you know a random tape you know will always be different from all the other tapes but but when this soup has matured um you you see um you know a lot of copies of this top one in this case again this was an 8,192 tape soup so 5,000 of the top one 297 of the next 99 of the next a long a kind of longtail distribution of of of tapes with different contents all of which appear to be copying themselves and each other So um so this is pretty this is pretty cool and it's exactly the emergence of these um uh you know universal copers uh that that has uh that has taken place from uh an environment that initially was just capable of doing computation but it was just random bytes. So uh we go from here to here um you know flat distribution only a few instructions and two operations per interaction to a very uh longtail distribution of tapes with thousands of operations per interaction and a lot more code on on the tapes. It's uh it's pretty cool. And uh this is what that phase transition looks like. So I'm I'm uh this is a scatter plot with 10 million points. Each point is an interaction. Um the uh the x-axis is time and you can see that in this case this phase transition occurs right at six million interactions. Um I'm calling it a phase transition because that's what it looks like. Uh you know the the uh the vertical axis here is the number of operations that run uh in each interaction. You can see that the number of operations just skyrockets. Um and um uh and and the other the other uh interesting indicator is that the compressibility of the soup uh becomes uh much much greater at that moment. So we start off with an incompressible soup. It's like a it's like a touring gas uh that is incompressible and at that moment of transition the soup becomes highly compressible because now suddenly you have lots of copies of everything. So you know I I uh following Walter Fontana I would call the phase of of matter on the left a touring gas gas because every every bite is decorrelated from every other bites. There's the the correlation function of the delta function and and the phase of matter on the right is not a standard phase of matter. It's not a liquid or a solid or a crystal. Um uh it might be what Schroinger called an aperiodic crystal in in um in his book what is life? But I would call that phase of matter life. It's complex. It's compressible. Uh but it has uh functional structure at every scale. It's it's not just it's not just repeats. Could I ask a question here to make sure I'm following along be? So you're doing a series. Each one of these is a is a computation. Uh there's a series in time, right? And so each computation leading to the next one. Is that correct? So there there are um two time scales here if you like. There's there's there's interactions. So, you know, pulling two random tapes out of the soup, concatenating them, and running, and then putting them back in the soup is an interaction. So, I'm showing here 10 million interactions. That's the x- axis. And um and and for every interaction, there is a point drawn and the y position of the point is is is the number of operations that ran in that interaction. Remember, you know, here I'm showing you, you know, right? The average number of operations that run in the beginning is two because you know there are only you know when when you take take two of these tapes and put them together you know there are very few operations here and generally you know there only two per tape. Um and and and there's you know likely to be a syntax error for instance when the first one you encounter is a closed bracket and there's no matching open bracket. Uh and then when when um you know after this transition there's a lot of computation happening and that's that's what you see here. Does that make sense? So yeah. Okay. And so then I'm trying to figure out where the path dependence is coming from. So are there any constraints at all between from one computation to the next? Um the only thing that that um that has that that has changed is that you know when when uh when two concatenated tapes run they can modify you know some other bite on the tape. Now in general you know at the beginning those those modifications are are very very minor like when there's a when there's a a dot that is that is a print or or a or a copy operation so that will copy a bite from one place to another and um and whenever there's a plus or a minus that will increment or decrement a bite whatever bite is ah so they're ex so they're not only you not only have a string of symbols here they're getting concatenated they're executing the instructions as well correct and beginning. This these self modifications are random and and and are very few in number because they're very few things that are even instructions and only an instruction can do a modification but but by the end that you have these long strings copying each other. So this is a little bit like Stuart Calfman's light bulbs connected with wires and you get to a certain point and poof it does something. Okay, gotcha. Thank you. That's right. Do you do you ensure the the programs are haunting at some point or Oh yes. Um that good question. Uh if you don't uh impose a maximum number of operations or a stochcastic probability of stopping then you will inevitably run into a infinite loop. So you you you have to do one one or both of those things. Okay. Yeah. I I just curious if you if you counting the total number of the the nonhoing or or something exceed your uh cap uh exceed your budget. Uh does it also have some sudden change around the phase transition point? It does. It does. Uh and it'll become clear why in a moment. But um but but yes, that's right. um you know there as you can see even before the um the this phase transition there are you know points that are very high and those that's because of of infinite loops um and uh yeah and so in this case um you know for making for making these plots I used a stochcastic stopping probability but actually in all of the um and I I did that for various reasons but in in all of the experiments that I'll show you from now on uh I used a thousand tape 1,024 tapes rather than 8,192 and a fixed 10,000 uh operation maximum rather than a stochastic halting probability. Um and I did that because I wanted to I wanted to remove all other sources of randomness. Um okay, so you know the suggestion here is that pretty much any universe that has a source of randomness and can support computation will evolve life if you if you grant that this kind of stuff is life. But there's a pretty big mystery here which is how do why does the the complexification happen? uh in particular when you don't add any mutation. So um you know I I originally in this setup in this system I had originally um uh introduced a a mutation rate because I assumed that you know evolution requires uh you know is Darwinian right that you get you get um mutation and selection um and and so we would need to have you know like one in one in 10,000 times a bite flips to a random value or something. So there was a mutation rate originally but um but but when I started to vary the mutation rate to see what if this moment of you know life cropping up. Um there is there is a relationship you know the the more mutation you add the faster life comes up to a point and then if you add too much mutation then you know then it then then you you kind of break the system. But the shock was that even with a mutation rate of zero you get this behavior you get this phase transition. So that's really a puzzle. How can you get evolution with no mutation? Uh so um so let's explore ask something. Yeah, of course. Um you you do need some randomness within associization, right? Because there might be some programs which cannot be combined in a way that you get reasonable results. No, no. I I I allow every uh every combination. So the in in the in the setup that in the setup that I'm going to show you, the only randomness is in the random choice of two programs to combine. Um and and and we but if you if you use the same program for instance to start with like a homogeneous pool then you you won't get anywhere right or am I getting something fundamentally wrong? Um we start off we start off with all of the tapes filled with random bites. Yeah. So, so the probability that two of them will be the same is effectively zero. Yes. Okay. And we pick two with uh without replacement. So, it's always a different one. Okay. Great. Thank you. Okay. So, uh so yeah, why does why does complexification happen without mutation is kind of the weird thing. Uh by now, first of all, you know, as as I think will be clear to all of you, by the end, we have a replicating entity and it can engage in standard population evolution dynamics. So, you know, it makes sense that that should be a more stable point than the random bites because, you know, things that can replicate persist through time. But how do we get there in the first place? So, um I'm going to just sketch the math of this and I'll I'll I'll try and skip over some stuff a little quickly because I I know we're running low on time, but in normal um you know, replication dynamics, you you think about a kind of very general onsets that looks like this. So you have a set of replicators uh you know x i um and um and their their rate of of change is some function of their current populations. Uh this is like a super general on subs that works for everything from chemical kinetics to population ecology. Um an example of such a system is like the lka volta uh predator prey dynamics which I'm sure many of you know about. Um th those look like this. So you've got two species. there is a prey species X1 and a predator species X2 and those terms correspond to reproduction uh and getting eaten for the for the prey and eating in order to reproduce and uh death for uh for uh for the predator and then you'll get these oscillations. Now um this is a more general form of that Lodka Voltera uh equation. So you know notice over here that there there are you know alpha and gamma are linear uh you know and they're they're the diagonal terms of a linear component and beta and delta are uh are bilinear right they've got they've got you know a product of x1 and x2. So this is just a general form of that um where r is um is the first order term and the second order the bilinear term is uh is parameterized by m and and b and and here's how you'd fill those in. There are no off diagonal terms in R becu uh in in lockabtera because uh in the logtera system you can't get the prey turning into a predator or the predator turning into a prey. So you have only diagonal terms. Uh I hope this makes sense. Now the the key thing here is that the linear part gives you reproduction and death and metamorphosis. You know predator turning to prey prey turning to predator which is not present here. And the bilinear part gives you predation, competition and finite niches. That's what limits the system and prevents these numbers from just blowing up to infinity. Okay. Now, we know that that that that this can't be the whole story of evolution because it's closedended. You know, in a system like Lava Voltera, you're never going to develop a third species and and and the whole point behind evolution is how do you get, you know, how do you get more species to arise? How do you get complexity out of such a thing? Well, um the uh the answer I think comes from um the the an idea introduced by Meshovski in the beginning of the 20th century and popularized by Lin Margulus uh which is uh symbioenesis. So symbioenesis is when two living things come together to make a new living thing and uh the question is could symbioenesis be happening in BFF. The answer is it does. Uh and the way you can see that it happens is by looking at the populations of all replicating strings in the soup from the very beginning. And by all replicating strings, I I I include strings as short as length one. I mentioned that you know there are uh certain instructions like the the the dot instruction print will basically print whatever is under the under head one onto head two. So that's already a one bite replicator if you want to think about it that way. And that's this first one that arises all all the way at the left. Um and what you see is more and more replicating strings um that that have faster and faster uh rise times and and decay times until this um phase transition which here happens at about 3.8 uh million interactions. Uh and and what's and when you look at those replicating strings and you track them closely, what you find is that is that the later replicating strings are combinations of the earlier replicating strings. In other words, if you have weak replicators that are, you know, replicating just single byes now and then once in a while two of those will end up together and then they'll replicate as a as a as a group. Uh so there's a merger tree of replicators and that's what results in these big complex programs. So it is symbioenesis that that drives the evolution. Uh so uh can we build symbioenesis into an ansot like the one that I just described? uh you can and you can do it with uh with the trick uh of Marian Smallhovski a statistical physicist who uh described a system uh called which he called coagulation dynamics uh in also in the early 20th century. So quagulation dynamics is is originally you know the statistical physics of things like forming scabs uh or um or gelatin uh where you have monomers that can come together to make druh to make quadr or whatever you call them and so on. So it's it's polymerization and and this is just uh the smoky quagulation equation it's really just bookkeeping. It says that when you have uh you know I + J turning into K uh the clusters here are are numbered by their size then uh you know that that reaction that that merging will happen proportional to the populations of I and J and to a merger a merger kernel KIJ and whenever such a merger happens for bookkeeping you have to then subtract the populations of the merging entities uh J and K. So you know that's that's uh merger gain and merger loss from you know everything that merged uh you know uh getting removed from the population. So um uh the cool thing is that Smallhovsky showed that these equations uh this this this equation for uh for coagulation produces a phase transition called gellation. It's a second order phase transition and that's exactly what happens when you put when you put um you know jell-o in the fridge as liquid and then it it turns into a gel uh you know overnight that's that's that phase transition and and the order parameter is the size of the largest cluster and basically when that reach when that shoots to infinity and then the whole thing turns into a gelatin that's gellation and that is exactly what is happening uh in uh in in the artificial life simulation. So basically as these little replicators stick together and they appear to do so faster and faster as you can see from the the um the increasing uh or the decreasing time scales eventually they all glom together and you get gilation. So it it literally is a phase transition it's smallky gilation. So uh if you put those two terms together uh one for uh replication and one for symbioenesis um you you have a general uh set of equations for um uh for replicators and in particular um you know the R part is closedended. It's normal population dynamics but the K part life forms out of the existing ones and and that gives you open-endedness. So um here is a uh you know a more uh worked out version of the replication part of the of the the line uh the the linear and bilinear part um for the artificial life system that I was just showing. Uh the reason that it that it has this particular form is that I'm giving you you know the general linear part on the left with R and the nonlinear part comes from the fact that when when a when some code copies itself it will overwrite uh some other code. And so I've introduced a um uh a uh an idea of niches which is basically just which bytes do things overwrite. Uh you know if if if one replicator lives in bytes three and four and another uh replicator lives in bytes 15 and 16 then no matter how much they copy each other their their niche overlap is zero. They won't compete. But if they if they copy themselves onto each other if they overlap in their niches then they will compete. One will overwrite the other. So you have this this niche term. the cyiogenesis part is is is more complex than the small hubski equations but that's only because we have to account for more than two things coming together. Uh you can have you know three or four or 17 replicators coming together. Uh so you need bigger you need you know uh kernels of of arbitrary size. Um and you have to account for the possibility that the the merged entity is actually a subset of the thing that got uh of the things that merged. It doesn't have to be all of those bytes. So that's what makes it look more complex, but it's just it's just the same thing. So that's that's all of those uh together, the the evolution part and the revolution part. Um and uh you can you can do a cool um intervention in this system by uh by tracking those mergers of replicators and by blocking uh mer certain mergers. So you know in this case uh and this is the important experiment and I'll I'll I'll conclude uh very soon. But here what what we do is um you you track the depth of the merger tree for every replicator that arises and we block mergers that result in in merger trees that are too deep. So you know these are you know depths uh limited by you know the yellow one is limited to depths of 128 then then uh you have the same behavior you had in the in the unclamped system. But if you limit depths to nine for instance, you allow only only uh mergers that are nine deep, then you don't get uh um galation uh and um and this involves only blocking maybe one in a thousand uh events. So the top curves are how many events do you have how many uh interactions do you have to block in order to prevent those uh those entities from forming. And you can see that in the case of nine uh you know it's it's not that many. It's only one in a thousand of these interactions that result in these novel deep uh um you know new entities. Um but that will prevent uh gilation from ever occurring. So um so in other words the moral is these rare events are really important that result in symbioenesis um and you need to have lots of symbioenesis to get to get to these uh these complex entities. Now the cool thing about doing that blocking is that you can also do a perturbation analysis. Uh this is what uh what the pop you know you've kept these populations of smaller uh replicators that um that then that plateau uh they're well fit by logistic functions and you can then look at how they are how their populations are correlated with each other as they fluctuate up and down. You can see that sometimes they appear to be cooperating uh the you know the the the fluctuations go up and down together like between the uh you know the orange and the pink one. uh and sometimes they appear to be anti-correlated because they're competing uh for the same niche and uh and so by doing a a bit of fluctuation analysis linearizing the dynamics around that steady state you can reconstruct the replication matrix. So here's what uh the replication matrix looks like for a typical one of these clamped runs. Uh and you can see uh three features that are important. One is that the diagonal is dominant and that tells you that mostly what's going on is replicators self-replicating just like in lotter but you also see off diagonal parts that are negative and that are positive. The negative parts are are uh largely symmetric about the diagonal and that tells you that if a competes with b then b also competes with a right. So, so competition is mutual but you also have positive components and those uh those off diagonal positive components are asymmetric. So this is cooperation among rep among replicators and they're not symmetric. The fact that A helps B doesn't mean that B helps A. Uh but you know A might help B which helps C which helps D which comes back around and helps A for instance. And it turns out that those complicated cooperations are indeed what what takes place. uh I I don't have time to explain the details but basically um the the higher you allow the the the merger depths to go the more unstable the system becomes the more ready to gelate you could see those time scales becoming faster and faster and that it's basically a cooperative instability where these replicators are becoming more and more cooperative with each other and and they're more and more ready to uh to to make that jump to uh to full gulation. So uh the moral of this story is that the R and K terms are related. They're not independent. Um you can see what is about to undergo symbioenesis based on what is already cooperating. So symbiosis is what leads to symbioenesis. It's almost as if symbioenesis is just a relabeling of symbiotic relationships. Lin Margulus used to talk about you know people as just bags of bacteria that are cooperating with each other. And I think she was right. uh you know we we are cooperating bacteria basically. Uh so um uh this is this is the answer to the question about about uh you know uh things that are not touring complete becoming touring complete later on. But basically um you know those those replicating bytes are definitely not vonoyman replicators. Uh the the the um the code that runs that copies the bite is different from the bite itself. Um but um but after when when things gelate you get this fusion of the of the code and the thing that it copies and and what you end up with are these cellular replicators in which the code that does the copying is part of what is copied. So um you know I I call inanimate replicators here um you know bytes that are completely disjoint from the code that does their copying. Viral replicators are are are times when the code that does the copying has an intersection with uh with the code that is copied but it's not fully contained and cellular are are cases where the code that does the copying is fully contained by the code that is copied. And basically what you can see is that right at the moment of galation cellular replicators take off. That's that's kind of what um uh what what that moment of gulation really is. It's the emergence of of of vonoan replication. Uh okay. Um so uh I I I want to just end by noting that uh there is lots of evidence that in um that that biology works this way too. Uh so you know this is just a breakdown of the um of the of our of our genome from from you know the 2001 human genome project. And um you know the the surprise from this work was that only one and a half% of our of our uh genes code for the proteins that make us up. And that so much of it are these lines, signs, LTR retrotransposons, DNA transposons, sequence repeats, segmental duplications, and all of that stuff is basically viral or transposon replicators that are operating below the level of the entire genome or or the entire organism. So uh you know my my view is that this is all evidence of our own genome having been uh constructed by subreplicators uh you know viruses and things like them as opposed to having arisen primarily through random mutation and selection. Uh so in other words, you know, we see evidence that, you know, obviously mutation does happen and Darwinian selection does happen, but but that symbioenesis has actually been a huge part of the way we have evolved too. And and there's by the way tons of papers in the last 10 years about how functional genes in our bodies have viral uh viral origins as well. So uh in this view, life has embodied autopoetic computation arising and complexifying through symbioenesis. Uh it's not just neuroscience that's computational. Life was computational from the start. It gets more computationally complex over time through symbioenesis at many scales and symbioenesis makes that computation massively parallel. So that means that you know cooperative computational parallelism enables collective intelligence via a division of cognitive labor. There's a there is an arrow of evolution here too, not just an arrow of time whereby things become more complex and become more computationally sophisticated as those computers uh learn to interact and re and replicate together. Um and that's that's what intelligence scaling laws tell us. This is my final slide. So uh you know in this view uh life and technology are the same. And uh and and there's a there's an implication here that that the kind of theory of complexity that many of us have been looking for uh is maybe not as far off as all that. Um we're doing a bunch of investigations of hierarchical information that should give us you know a kaggoro or algorithmic complexity view of how life becomes more complex. Essentially every time things come together you're adding bits of information regarding how they come together. And that's that's the origin of complexity in the code. Um and um and and as far as platonic ingressors go, if you think about dynamically stable cycles, whether of those very simple kinds or the the kinds that start to interlock to form more complex replicators, uh if you if you do a ponare map of those things, then you turn those cycles into into into points, right? And and those are um if you like the identities of different possible platonic ingressors. Um, if you consider the symbiogenetic multiverse, right, of everything that could have combined, then that gives you a universe of possible things that combine together and our contingent history is basically which ones have combined and resulted in what we've got today. Uh, and that results in in in open-endedness. Uh, right, the fact that every time you combine, you have a new basis for things to combine further. Uh, all right. Phew. I'm sorry. I I I really tried to race there to get us there within within an hour. I know that was a little bit a little bit uh uh too quick at the end to probably make sense. Um I don't know if we have any uh any time for um Sure. uh for additional uh you know questions discussion. Uh I can I can definitely hang out for a little bit. That's great. Yeah. Yeah, let's absolutely do that. Yeah. Um Douglas, hi. So from a biohysicist point of view and somebody who's looked at evolution a lot, I like it all. It all makes sense to me. You were talking at the beginning about purpose and agency and cybernetics and so you've shown now that you can get structure without a purpose but cybernetics the key thing is that there's a goal programmed in there somewhere. So are you saying that you don't really need it or that there's an extrapolation from what you're talking about that will get us to goals? Yeah. No no I I think what I've shown is actually the emergence of purpose. So um you know in in the same sense that like even even when you look at a program that is replicating itself in others right that that has that now has purpose its purpose is to replicate itself in others if you break it unlike a rock right by flipping a bite or something it will no longer replicate it's now broken in the same sense that you know a kidney that has failed has broken so I you know what what I'm showing is the emergence of purpose out of out of a purposeless uh universe if that makes sense. Okay. So, so the al Oh, sorry. Go ahead. Please. So, so the so you would say then the purpose is species survival and then that will then generate sub goals of like getting into the car and driving into work and so forth. Exactly. Exactly. Purpos lead to more purposes. Every time every time two things serve a function for each other, every time there's cooperation, you get a multiplication of purposes if you like. right there. You know, more purposes emerge as things cooperate. Okay, thanks. Who's next? Let's see. Akar. Yeah. Uh great talk. I also saw your talk at a life. Uh I like both of them. Um my question was um on basically the emergence of these um um touring complete like uh universal constructors because in your experiments um the touring like the computation nature of it was kind of baked into the language of brain right um I was wondering I' I've been doing a lot of experiments with like particles and like particle based simulations or like grid based simulations how do you expect um how do you think that the um these different substrates which don't have like computation in the traditional touring sense baked into them. How can they emerge these universal constructors and um life? I mean that is the question of origin of life but u more concretely is there like do you how do you see the progression from just particles to creating something like DNA that can do universal computation within like let's talk about a particle based simulation specifically I guess to keep it concrete. Yeah. So so I mean it's a great question. um you know the the number of of physical universes in which you can build a computer is you know very large right that's a low bar and that's something that I think you know you know wolfro and cook at all have shown you know and enumerating all of the possible solar automata for instance so you know if you think about fundamental laws of physics that are really simple you know that just involve like you know uh game of life type stuff right if my you know three of my neighbors are black then I turn black or something um the you know there are many many many such the majority of those in some sense perhaps are capable of computation. So uh when you when you define a computer there's always this question of of how you are partitioning uh you know what is the physics and what is the definition of the computer. So you know here I've I've made a a kind of generous partition for you know where the physics includes an entire touring complete language. Um but you know if you make the partition very very you know if you move that partition to a very different spot and you say you know it's just a really simple cellular automaton you know my point here is that you will still get you know all of the phenomena that I just described it's just those first steps involve you know figuring out a set of of of patterns or of instructions that can that can engage in symbiosis together in order to generate a touring complete language which you then need to make a universal constructor. So um if I guess if it's about how you course grain I see if the physics bakes in less um computation then the life will have to evolve more to get that umism. I see. Exactly. Exactly. So it's about it's I'm starting a little way up the ladder where you know the you know hidden in in that set of things are you know eight instructions that work together to make a touring complete language along with you know a whole bunch of bytes that don't do anything. But um but if you had, you know, just pixels, you know, that can fit together to make a machine that can work with other ones, it's the same process. You just you're starting a little lower on the ladder. Okay. Thank you. All right, David. I'm interested in the applications of this for uh bioelectric networks and how um collections of cells might follow a purpose to build an organism to build a part of an organism to build an eye. Can you tell me how all this might be represented in a or or you you know how how this could be represented in a bio electric network to sort of relate it to Michael Leven's work on this topic and the idea of agency and purpose in morphagenesis. Well, um, one one thing that that it becomes clear when you look at stuff this way is that there's agency or purpose at every level of that of that symbioenetic uh, merger tree. So, um, you know, I'm not sure I can relate I can't relate it directly to the, you know, electrical morphagenesis stuff that Mike has been doing, although I I think we could we could build, you know, a bridge between those two. But but but if you if you take, you know, Mike's kind of fundamental point to be, you know, that that we're not just like uh collections of of mindless robots, but actually collections of things that are themselves intelligent, which are collections of things that are themselves intelligent, you know, that are cooperating, right, in order to in order to make the larger thing, then I think this genetic perspective is very consistent with uh with that view. Um and um and the fact that so much of what appears to have been our evolution is based on in you know essentially infectious genes right or or or or you know things that already had function that merged their way into us as opposed to these random mutations you know tells us that not only at the level of development but also at the level of evolution there is a purposiveness uh you know in in the way those parts have combined you know just to give a random example um the placenta you know is uh you know is made possible by sincen which is actually a viral code protein um from a virus that endogenized you know into our germ line around around the around the time that uh you know that mammals uh came on the scene and in fact there's a whole family of placental proteins that come from different viruses that have that have indogenized at various times. So you know we we are actively constructed not just we didn't just randomly chance into into being by simpler things. All right, Hanel. Um, this is just a small technical question. I'm I'm really surprised that you don't need a mutation. Somehow it's bothered me and and I I'm asking maybe the way that you started the the machine is the randomness that I'm looking for. In a way, it's a great question. You absolutely need a source of randomness. Um and and what's what's serving that function here is two things. First, we began with with um with 1,00 64 random bytes, right? So there's 64,000 bytes. That's that's the endowment of initial noise if you like. Um now that's not enough on its own to, you know, for there to be like a complex self-replicating program in there. Uh but but it is essential because it's enough for there to be one bite replicating programs somewhere in there. And if you don't have those, then you you can't get off the ground. The other source of the other source of randomness that's really critical is the random selection of two tapes out of the soup to combine. That random selection will once in a while result in a conjunction of things that that um that work together and stick. So you can think of the complexity of the code that arises as being the fossilized uh you know um uh you know choices that worked, right? So it's it's a different version of of of mutation basically where you know things colliding right things happening to collide that work are the you know are actually the basis for the complexity of of the the komaal complexity of what of what comes out the other end. So you know it's not mutation in the conventional sense um but it is a chance-based building up of complexity right okay so so if you would start I don't know 60 tapes copy of the same one you would not get any advancement right that's right nothing would happen and and by the way you can think of mutation as just being more symbioenesis right you can think of it as being you know uh if you like the introduction into the soup of of single bites uh that you know that that that enter into a symbioenesis that may or may not work with the larger thing. Those can be productive. Sometimes they can speed it up but but the point is that larger things combining are more powerful. Uh you know which is why um you know it's it it's it's not that it becomes a less and less important driver as you get further along evolution. Yeah. Okay. Thank you. Uh let's see. Akash. Yeah. Uh I was wondering so uh since uh brain captures this idea of like symbio sim biogenesis so well and like the origin of life almost what would happen if you just ran the system for like trillions of steps and like a really big grid. Do you expect like open-ended evolution? Um, great question. In the way that we want it to just like to just come out of it and if so, we should be investing a lot of compute into this, right? I agree. I agree. Um, it won't work as it is. Um, and and the reason the reason is twofold. Uh, space and energy. So, um, you know, the the the limitation BFF um, you know, gets us, you know, you're right. It points the it points the path toward open-endedness because you can imagine like I could just keep combining things together and get more and more complexity. But it's this particular setup is limited for two reasons. Uh one of them is that the tapes are fixed of length 64. Um and and there's no structure among the tape. They can't form any higher order structures because you know it's it's just a random soup. You know there's no ability for these things to form larger compartments. The other limitation by the way is is on number of computing steps, right? So, so there's an energy limitation too. So, we need to redesign the BFF setup in a way that allows those um those programs to create larger niches and and and to combine into larger and larger structures beyond that limit of of 64. And then I think we'll have something open-ended and then that will be a very interesting experiment indeed to do. So, that's that's an active area of work. But if you have ideas about how to about about how to change it to make it open-ended, you know, all that that are different from mine, I'm all yours. But yes, it it does point the path toward open-endedness. And I agree those are very interesting experiments to do. Doug, yeah, two comments that might help bridge the computer science world and the biology world. One is there's a literature in computer in um cancer biology where people were looking at genome rearrangements. uh in cancer cells and then what they would do is clone out the individual progeny. To make a long story short, what happens is for many passages you get all kinds of rearrangements but eventually it settles into one arrangement and there are no changes after that. It's just stable and which final one you settle on depends on which lineage you look at. So there are lots of possible solutions but there's an evolution towards a stab stable changes of the kinds large scale kinds of things that you're talking about. The other one is a is a beautiful paper which I have looked for I can't find about 25 years ago in nature and they were asking how wasp build nests and so this has to do with your embodied computation. What happens is the wasp brain doesn't know what a wasp nest looks like. What happens is it lands on a partially built nest and if it sees a wall over here, it spits there. If it sees a wall over there, it spits there. And the net algorithm of the existing nest plus the wasps creates a nest. That's very cool. And and and similar in many ways to how morphagenesis works inside the body probably um you know during development. Uh so as a kind of external development um all right apolog